{
 "cells": [
  {
   "cell_type": "raw",
   "id": "62c002f7-67df-4741-99be-bd133ac65c6a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A cheatsheet for setting priors on GPs\"\n",
    "author: \"Fizz McPhee\"\n",
    "date: \"5/22/2021\"\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b2c1f0-b9d8-4821-97c4-292a8d9ea2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import nutpie\n",
    "\n",
    "RANDOM_SEED = 99999\n",
    "rng = np.random.default_rng(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c9b53-e60a-483b-b0db-163b1160150c",
   "metadata": {},
   "source": [
    "# A cheatsheet for setting priors for Gaussian process hyperparameters\n",
    "\n",
    "People fuss a lot of priors in Bayesian models.  Insightful, right?  The thing is though, in a lot of Bayesian models you can set some pretty reasonable priors without thinking that hard about it.  Your sampler will run fine and your results will look alright.  Gaussian processes are not in this category.  Most of the time how you choose your priors will make or break the model and so some serious fussing is required in order to operate these things correctly.  This post is here to help you guide your fussing.  \n",
    "\n",
    "The most similar resources out there to this post are:\n",
    "- Micheal Betancourt's post [Robust Gaussian Process Modeling](https://betanalpha.github.io/assets/case_studies/gaussian_processes.html).\n",
    "- Dan Simpson's [priors for the parameters of Gaussian processes](https://dansblog.netlify.app/posts/2022-09-07-priors5/priors5.html).\n",
    "\n",
    "These posts are both excellent and give super detailed explanations and the mathematical \"whys\" behind their reasoning.  Not to mention the fact that it's their original work we're using here.  However I will boldly assert that the problem with them is that they have lots of words and sometimes things with lots of words take a long time to read and understand, especially when GPs are involved.  My goal here will be to try and distill out the practical pieces from these two (and other) resources so this post can be used as more of a cheatsheet for people trying to fit GPs in practice.  If you want to actually learn things, start here and then be sure to go there afterwards. \n",
    "\n",
    "#### Disclaimer #1\n",
    "\n",
    "This note applies to fitting GPs using MCMC in a [PPL](https://en.wikipedia.org/wiki/Probabilistic_programming) like PyMC, Stan, or NumPyro.  If you're trying to fit GPs using optimizers in something like GPFlow, GPJax, or GPyTorch, this advice is still very applicable -- but it's not directly targeted to that scenario.\n",
    " \n",
    "#### Disclaimer #2\n",
    "\n",
    "This note also only applies to GPs with covariance functions, or kernels, in the Mat√©rn family which all have a lengthscale hyperparameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b685f8-f7d4-4829-86e9-2c2f985b973d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
