[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What about this blog"
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html",
    "href": "posts/occams-razor/occams-razor.html",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "",
    "text": "import pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nseed = 12345678\nrng = np.random.default_rng(seed)\nI saw this quote in Gelman’s blog recently that reminded me of a cool example I saw awhile back from this short paper titled “Occam’s razor” from NIPS 2000. The example demonstrated that fitting an unknown function using a large number of basis function with certain regularizing priors was preferable to casting the problem as model selection and choosing the optimal number of basis vectors. It helped it click for me why non-parametric model components like Gaussian processes often work better than parametric models with a fixed set of features like polynomials or splines. The quote (from Radford Neal in the context of Bayesian neural networks) is:\nThe goal of this post is to reproduce that example in a modern context by building the model in PyMC, sampling with NUTS, and using leave-one-out (LOO) crossvalidation for model selection.\nYou’ve probably seen an example like this in textbooks: Given a simple scatterplot, fit a curve to it and use model selection to determine the right number of basis functions to use. You’re usually choosing the best polynomial order. The example is designed to demonstrate the bias variance tradeoff. Or alternatively you can use the Bayesian evidence to select the right polynomial degree.\nI think what Neal is saying is that it’s a mistake to transform the original problem into a new one, where the new goal is to find the “right” number of basis functions. It’s better to address the original problem directly, where the goal is to get a parsimonious representation of the unknown function. No one ever actually cares about the best number of basis vectors to use. Transforming the problem creates an unnecessary step."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#the-two-experiments",
    "href": "posts/occams-razor/occams-razor.html#the-two-experiments",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "The two experiments",
    "text": "The two experiments\nWe’re going to run two experiments. First, and you’ve probably seen before, we’re going to build a sequence of polynomial regression models with increasing complexity and then choose the “best” model, according to the leave-one-out crossvalidation (LOO-CV) metric. We’ll use the exact same priors for the polynomial coefficients.\nThen in the second experiment, we’ll do the exact same thing, except, we’ll keep decreasing the prior scale we use for the polynomial coefficients as we add more terms. Instead of the number of basis vectors, we’re using our priors as the hedge against overfitting. Then we’ll see what LOO-CV says is the best model.\nFor our experiment, here’s some data that’s generated from a step function with a bit of additive Gaussian noise.\n\nx = np.linspace(0, 2, 50)\nf = np.ones(50)\nf[:25] = 0.0\ny = f + 0.3 * rng.normal(size=len(x))\n\nplt.plot(x[:25], f[:25], color='k', label=\"underlying function\")\nplt.plot(x[25:], f[25:], color='k')\nplt.plot(x, y, 'o', label=\"observed data\");\nplt.xlim([0, 2]);\nplt.xlabel(\"x\");\nplt.ylabel(\"y\");\nplt.legend(loc=\"lower right\");\nplt.title(\"Our example dataset\");\n\n\n\n\n\n\n\n\n\nDetails\nBefore getting going and actually modeling this, there’s one preliminary thing to mention. Feel free to skip over this bit about the QR decomposition because it’s not really core to the goal here. We just need it to make our priors on the polynomial coefficients comparable and on the same overall scale.\nWe’ll use the QR decomposition to orthogonalize our basis. I found some Python code to do this here, pasted below.\n\ndef ortho_poly_fit(x, degree = 1):\n    n = degree + 1\n    x = np.asarray(x).flatten()\n    if(degree &gt;= len(np.unique(x))):\n            stop(\"'degree' must be less than number of unique points\")\n    xbar = np.mean(x)\n    x = x - xbar\n    X = np.vander(x, n, increasing=True)\n    q,r = np.linalg.qr(X)\n\n    z = np.diag(np.diag(r))\n    raw = np.dot(q, z)\n\n    norm2 = np.sum(raw**2, axis=0)\n    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]\n    Z = raw / np.sqrt(norm2)\n    return Z, norm2, alpha\n\nThe left panel below shows the first four polynomial basis vectors and gives their correlation matrix. The right panel shows the same four basis vectors after they’ve been orthogonalized.\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\ndegree = 3\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10.0, 4.0])\n\n## SUBPLOT 1\nX = np.vander(x, degree + 1, increasing=True)[:, :degree+1]\nax1.plot(x, X, lw=2);\nax1.set_xlim([0.0, 2.0])\nax1.set_ylim([-0.2, 8.0])\n\naxins1 = inset_axes(ax1, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nXcorr = np.corrcoef(X[:, 1:].T)\naxins1.set_xticks([])\naxins1.set_yticks([])\naxins1.imshow(Xcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Xcorr[j, i]\n        axins1.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"white\")\n\n## SUBPLOT 2\nZ, _, _ = ortho_poly_fit(x, degree=degree)\nax2.plot(x, Z, lw=2);\nax2.set_xlim([0.0, 2.0])\nax2.set_ylim([-0.5, 0.9])\n\naxins2 = inset_axes(ax2, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nZcorr = np.corrcoef(Z[:, 1:].T)\naxins2.set_xticks([])\naxins2.set_yticks([])\naxins2.imshow(Zcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Zcorr[j, i]\n        axins2.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"c\")\n\n\n\n\n\n\n\n\nThere are other benefits to orthogonalizing the basis, but our main reason is to have all the basis function coefficients living on the same scale. Notice in the left panel the basis functions range from [0, 2], [0, 4] and [0, 9], so scale-wise they’re all over the place. A lesser reason (for our purposes here) is that the basis functions are now orthogonal, which makes things quite a bit easier on the sampler. The two inset panels show the correlation matrix of the 3 non-constant basis vectors."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#bonus-points",
    "href": "posts/occams-razor/occams-razor.html#bonus-points",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "Bonus points",
    "text": "Bonus points\nWhat about gamma, how did we choose that? The Occam’s razor paper also talks about this (at the start of Sec. 3). This time we’ll just run it once as a 30 degree polynomial (which is only 5 less than the total number of data points!). We should get the same ELPD. Yes, this is how I chose the magic number gamma = 1.25 above.\n\nwith pm.Model() as model:\n    degree = 30\n    Z, _, _ = ortho_poly_fit(x, degree=degree)\n   \n    gamma = pm.Exponential(\"gamma\", scale=1.0)\n    scale = 5.0 * np.arange(1, degree + 2) ** (-gamma)\n    beta = pm.Normal(\"beta\", mu=0.0, sigma=scale, size=degree + 1)\n   \n    mu = pm.Deterministic(\"mu\", Z @ beta)\n    sigma = pm.Exponential(\"sigma\", scale=5.0)\n    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n        \n    idata = pm.sample(nuts_sampler=\"nutpie\", target_accept=0.9)\n    pm.sample_posterior_predictive(idata, progressbar=False, extend_inferencedata=True)\n    pm.compute_log_likelihood(idata, progressbar=False)\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00  Chains in warmup: 0, Divergences: 0]\n    \n    \n\n\nSampling: [y]\n\n\n\naz.plot_trace(idata, var_names=\"gamma\");\n\n\n\n\n\n\n\n\n\nelpd_data = az.loo(idata)\nprint(f\"ELPD: {elpd_data.elpd_loo:0.2f}, ELPD SE: {elpd_data.se:0.2f}\")\n\nELPD: -14.71, ELPD SE: 5.88\n\n\n\nall_idatas = {**idata_results1, **idata_results2, **{\"Prior on gamma\": idata}}\n\n\naz.compare(all_idatas).head(10)\n\n/Users/andre/miniconda3/envs/dev/lib/python3.11/site-packages/arviz/stats/stats.py:309: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'True' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n/Users/andre/miniconda3/envs/dev/lib/python3.11/site-packages/arviz/stats/stats.py:309: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'log' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nPrior on gamma\n0\n-14.713950\n11.646827\n0.000000\n0.614029\n5.884088\n0.000000\nTrue\nlog\n\n\nExp 2, degree 7\n1\n-17.903478\n7.316925\n3.189529\n0.272241\n5.682735\n8.401857\nFalse\nlog\n\n\nExp 1, degree 7\n2\n-17.990226\n8.009275\n3.276277\n0.113730\n5.163729\n8.268976\nTrue\nlog\n\n\nExp 2, degree 9\n3\n-18.163848\n7.680631\n3.449898\n0.000000\n5.568024\n8.282049\nFalse\nlog\n\n\nExp 2, degree 8\n4\n-18.203050\n7.603282\n3.489101\n0.000000\n5.602184\n8.322322\nTrue\nlog\n\n\nExp 2, degree 12\n5\n-18.206949\n8.157549\n3.493000\n0.000000\n5.197726\n8.023377\nTrue\nlog\n\n\nExp 2, degree 11\n6\n-18.276150\n8.165427\n3.562200\n0.000000\n5.300492\n8.106990\nTrue\nlog\n\n\nExp 2, degree 10\n7\n-18.485222\n8.217779\n3.771272\n0.000000\n5.498343\n8.270510\nTrue\nlog\n\n\nExp 2, degree 27\n8\n-18.530088\n10.165056\n3.816139\n0.000000\n5.392681\n8.231695\nFalse\nlog\n\n\nExp 2, degree 19\n9\n-18.534833\n9.554110\n3.820883\n0.000000\n5.349856\n8.187055\nFalse\nlog\n\n\n\n\n\n\n\nThis shows that putting a prior on gamma actually gives the best model in this situation, by far."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#summary",
    "href": "posts/occams-razor/occams-razor.html#summary",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "Summary",
    "text": "Summary\nWe’ve essentially transformed a discrete prior on model complexity via the choice of the number of basis vectors, to a continuous prior on model complexity, represented by the gamma parameter. This example nicely demonstrates that the common example you’ve probably seen about model complexity and the bias variance trade-off isn’t really the whole story.\nNeal argues that it’s better to use a complex model where the simple model is a special case, and let the data decide. Gaussian processes are really good at doing this. In particular, the HSGP approximation looks almost exactly like this.\nThere’s also a relationship to penalized splines. There’s a nice case study in Stan and an implementation here in PyMC. The same underlying concept is at play here too. The idea is not to spend too much time choosing the number and location of knots, but instead choose “too many” knots and rely on the coefficient priors to control model complexity. For spline coefficients, we also care about smoothness, in addition to how they prior penalizes their distance from zero. Flatter functions will all have more similar spline coefficients."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Priors, polynomial regressions, and ‘Occam’s Razor’\n\n\n\n\n\n\nbayesian\n\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\nBill Engels\n\n\n\n\n\n\nNo matching items"
  }
]