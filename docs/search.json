[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Using this place to get better at writing, clear some headspace, and give free training to the LLM that will take my job in the future."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html",
    "href": "posts/occams-razor/occams-razor.html",
    "title": "The Bias-Variance tradeoff vs. ‘Occam’s Razor’",
    "section": "",
    "text": "import pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nseed = 12345678\nrng = np.random.default_rng(seed)\n\n# silence PyMC logging\nimport logging\nlogger = logging.getLogger(\"pymc\")\nlogger.handlers.clear()"
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#the-bias-variance-tradeoff",
    "href": "posts/occams-razor/occams-razor.html#the-bias-variance-tradeoff",
    "title": "The Bias-Variance tradeoff vs. ‘Occam’s Razor’",
    "section": "The bias / variance tradeoff",
    "text": "The bias / variance tradeoff\nThe bias / variance tradeoff is a tentpole of statistical learning theory. Here’s the wikipedia entry if you need a refresher. The gist of it is:\nAdding more parameters to a model increases its ability to fit the training data set which decreasing the overall bias of the model’s predictions. Then, if you were to resample and refit the training set, the model’s parameters will be more different each time, increasing the variance in that sense.\nThe usual advice that follows is to use model selection techniques to try and find a balance between the complexity of your model and the fit on your training data set. This blog post is about how to sidestep the model selection problem when we are in a Bayesian context by thinking carefully about our priors."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#the-subtle-part",
    "href": "posts/occams-razor/occams-razor.html#the-subtle-part",
    "title": "The Bias-Variance tradeoff vs. ‘Occam’s Razor’",
    "section": "The subtle part",
    "text": "The subtle part\nI had seen this quote in Gelman’s blog awhile back that reminded me of a cool example I saw years and years ago from this short paper titled “Occam’s razor” from NIPS 2000. Andrew Gordon Wilson also demonstrates this with a similar example in his thesis.\nThat paper showed that fitting an unknown function using a large number of basis function with particular regularizing priors was preferable to casting the problem as model selection and choosing the optimal number of basis vectors.\nThe example in that paper made it click for me why non-parametric model components like Gaussian processes often work better than parametric models with a fixed set of features like polynomials or splines. I also use this idea when I’m choosing which features – actual features, not basis functions – to consider adding into regression models. So the quote, which is from Radford Neal in the context of Bayesian neural networks, is:\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nThe goal of this post is to illustrate this idea by reproducing the example in the “Occam’s Razor” paper, but in a more modern software context using PyMC, sampling with NUTS, and finally using leave-one-out (LOO) crossvalidation for model selection.\nYou’ve probably seen an example like this in textbooks: Given a simple scatterplot, fit a curve to it and use model selection to determine the right number of basis functions to use. You’re usually choosing the best polynomial order. The example is designed to demonstrate the bias variance tradeoff. Or alternatively you can use the Bayesian evidence to select the right polynomial degree.\nI think what Neal is saying is that it’s a mistake to transform the original problem into a new one, where the new goal is to find the “right” number of basis functions. It’s better to address the original problem directly, where the goal is to get a parsimonious representation of the unknown function. Most likely no one will ever care about the best number of basis vectors to use. By turning your modeling problem into a model selection problem, you’ve made your work harder."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#details-details-details",
    "href": "posts/occams-razor/occams-razor.html#details-details-details",
    "title": "The Bias-Variance tradeoff vs. ‘Occam’s Razor’",
    "section": "Details, details, details",
    "text": "Details, details, details\nBefore getting going and actually modeling this there’s one preliminary thing to mention. Feel free to skip over this bit about the QR decomposition because it’s not really core to the goal here. We just need it to make our priors on the polynomial coefficients comparable and on the same overall scale.\nWe’ll use the QR decomposition to orthogonalize our basis. I found some Python code to do this here, pasted below.\n\ndef ortho_poly_fit(x, degree = 1):\n    n = degree + 1\n    x = np.asarray(x).flatten()\n    if(degree &gt;= len(np.unique(x))):\n            stop(\"'degree' must be less than number of unique points\")\n    xbar = np.mean(x)\n    x = x - xbar\n    X = np.vander(x, n, increasing=True)\n    q,r = np.linalg.qr(X)\n\n    z = np.diag(np.diag(r))\n    raw = np.dot(q, z)\n\n    norm2 = np.sum(raw**2, axis=0)\n    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]\n    Z = raw / np.sqrt(norm2)\n    return Z, norm2, alpha\n\nThe left panel below shows the first four polynomial basis vectors and gives their correlation matrix. The right panel shows the same four basis vectors after they’ve been orthogonalized.\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\ndegree = 3\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10.0, 4.0])\n\n## SUBPLOT 1\nX = np.vander(x, degree + 1, increasing=True)[:, :degree+1]\nax1.plot(x, X, lw=2);\nax1.set_xlim([0.0, 2.0])\nax1.set_ylim([-0.2, 8.0])\n\naxins1 = inset_axes(ax1, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nXcorr = np.corrcoef(X[:, 1:].T)\naxins1.set_xticks([])\naxins1.set_yticks([])\naxins1.imshow(Xcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Xcorr[j, i]\n        axins1.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"white\")\n\n## SUBPLOT 2\nZ, _, _ = ortho_poly_fit(x, degree=degree)\nax2.plot(x, Z, lw=2);\nax2.set_xlim([0.0, 2.0])\nax2.set_ylim([-0.5, 0.9])\n\naxins2 = inset_axes(ax2, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nZcorr = np.corrcoef(Z[:, 1:].T)\naxins2.set_xticks([])\naxins2.set_yticks([])\naxins2.imshow(Zcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Zcorr[j, i]\n        axins2.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"c\")\n\n\n\n\n\n\n\n\nThere are other benefits to orthogonalizing the basis, but our main reason is to have all the basis function coefficients living on the same scale. Notice in the left panel the basis functions range from [0, 2], [0, 4] and [0, 9], so scale-wise they’re all over the place. A lesser reason (for our purposes here) is that the basis functions are now orthogonal, which makes things quite a bit easier on the sampler. The two inset panels show the correlation matrix of the first 3 non-constant basis vectors."
  },
  {
    "objectID": "posts/how-to-fit-a-gp/Untitled1.html",
    "href": "posts/how-to-fit-a-gp/Untitled1.html",
    "title": "Bill Engels's Web Log",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport pytensor.tensor as pt\nimport arviz as az\nimport nutpie\n\nRANDOM_SEED = 8998\nrng = np.random.default_rng(RANDOM_SEED)\n\n\nGB brand sellout vs. my branch vs develop, no price elasticity tvp in either\nUS brand sellout same comparison Now check in with daniel. because recruitment on dev and sklearn dont match and we dont know whcih ones right Now maybe branch tvp elasticity off sklearn instead of develop. depending on what daniel finds is the correct branch, dev or sklearn\nGB and US recruitment same comparison\n\ncompare relative_contributions.md file when you run a script for azure. good to do this testing on azure. this is the highlevel check, there are some other scripts to run if this check fails but figure that out then.\n\nf = np.random.randn(10_000)\nf_t = pt.softplus(f).eval()\n\nplt.hist(f_t, 100);\n\n\n\n\n\n\n\n\n\nnp.mean(f_t), np.median(f_t)\n\n(0.8174679974235869, 0.7086750319624299)\n\n\n\nf = np.random.randn(10_000)\nf_t = pt.exp(f).eval()\n\nplt.hist(f_t, 100);\n\n\n\n\n\n\n\n\n\nnp.mean(f_t), np.median(f_t)\n\n(1.6321055023796176, 0.9999809835924098)\n\n\n\nimport pytensor.tensor as pt\n\n\npt.set_subtensor(pt.ones((5, 4))[:, -1], 0.0).eval()\n\narray([[1., 1., 1., 0.],\n       [1., 1., 1., 0.],\n       [1., 1., 1., 0.],\n       [1., 1., 1., 0.],\n       [1., 1., 1., 0.]])"
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html",
    "title": "Intro",
    "section": "",
    "text": "---\ntitle: \"My Post\"\ndescription: \"Post description\"\nauthor: \"Fizz McPhee\"\ndate: \"5/22/2021\"\ndraft: true\n---\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport pytensor.tensor as pt\nimport arviz as az\nimport nutpie\n\nRANDOM_SEED = 8998\nrng = np.random.default_rng(RANDOM_SEED)"
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#the-lengthscale-prior-is-important",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#the-lengthscale-prior-is-important",
    "title": "Intro",
    "section": "The lengthscale prior is important",
    "text": "The lengthscale prior is important\nBefore getting into, I want to show how using uninformative / bad lengthscale priors can cause problems. To demonstrate this, I’ll draw some prior samples from a GP where the lengthscale has a uniform prior from 0 to 100, and the x data is daily, going from 0 to 100 days.\nWhat happens under the hood is: 1. Draw one sample from the lengthscale prior 2. Plug that into the covariance function 3. Calculate the covariance matrix over all pairs of x 4. Draw one sample from a multivariate normal with that covariance matrix 5. Repeat\nSo each of the samples below is a sample with a different lengthscale.\n\nx = np.arange(100)\n\nell = pm.Uniform.dist(lower=0.0, upper=100.0)\ncov = pm.gp.cov.Matern52(1, ls=ell)\n\nn_samples = 25\ns = pm.draw(pm.MvNormal.dist(mu=np.zeros(len(x)), cov=cov(x[:, None])), draws=n_samples, random_seed=rng)\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\nax.plot(x, s.T);\n\nax.set_xlim([0, 100]);\nax.set_xlabel(\"x\");\nax.set_title(f\"{n_samples} samples from a GP with $\\ell \\sim U(0, 100)$\");\n\n\n\n\n\n\n\n\nThese are look very different, in terms of wiggliness. Some of the samples from the GP change very slowly and some change very rapidly. Unless you’re really fishing, you probably already know quite a bit about the rate at which changes are happening. TikTok may make substantive or relevant changes to their algorithm on roughly a monthly time scale, instead of annual, or by the minute. A particular ad campaign may run for a few weeks at a minimum. These sorts of insights help put priors on the lengthscale (which has units as x).\nIf you stare at it a while, you might also notice that there are a lot more very smooth, slowly varying lines than there are wiggly, quickly changing lines. This is because a uniform prior on the the lengthscale doesn’t translate to uniform “wiggliness”. This is why the uniform prior isn’t a good prior. We can’t quantify the wiggliness of each sample by calculating the number of zero crossings of each sample. A very smooth sample may cross zero once, or not at all. A very wiggly sample will cross zero many times. I think the number of zero crossings is a better measure for how wiggly a function appears to be. Keep in mind that the number of zero crossings will of course depend on the domain of x. In this case x runs from 0 to 50 days.\n\nn_samples = 1000 # draw 1000 samples from a GP with the uniform lengthscale\ns = pm.draw(pm.MvNormal.dist(mu=np.zeros(len(x)), cov=cov(x[:, None])), draws=n_samples, random_seed=rng)\nzcs = (np.diff(np.sign(s), axis=1) != 0).sum(axis=1) # calculate zero crossings of each sample\n\nplt.hist(zcs, bins=np.arange(50));\nplt.xlabel(\"Number of zero crossings in 50 days\");\nplt.ylabel(\"Number of GP samples\");\nplt.title(\"Count of zero crossings, Unif(0, 100) prior\");\n\n\n\n\n\n\n\n\nWe can clearly see that the number of zero crossings is strongly skewed towards zero. So gradually, slowly changing samples are strongly represented. The point of this was to show that while lengthscale controls wiggliness, it doesn’t equal it. If it did, we might expect a uniform prior on lengthscale to give us a uniform distribution of zero crossings, but that clearly isn’t the case.\nBefore moving on, let’s try out the inverse gamma prior we said at the beginning was the best. Imagine we’re roughly aiming for weekly variation, maybe because new products are introduced every week. Let’s set a prior on the lengthscale that’s inverse gamma, where 90% of the prior mass is between 5 and 10, to roughly give us weekly variation. We can use the built-in function pm.set_constrained_prior to help us do that.\n\n# why inverse gamma rather than normal \n\n\npm.find_constrained_prior(\n    pm.Normal,\n    lower=4, upper=11, mass=0.95,\n    init_guess={\"mu\": 7, \"sigma\": 2},\n)\n\n{'mu': 7.499959574903695, 'sigma': 1.7857473928234064}\n\n\n\nell_params = pm.find_constrained_prior(\n    pm.InverseGamma,\n    lower=4, upper=11, mass=0.95,\n    init_guess={\"mu\": 7, \"sigma\": 2},\n)\nell = pm.InverseGamma.dist(**ell_params) \ncov = pm.gp.cov.Matern52(1, ls=ell)\n\nn_samples = 1000\ns = pm.draw(pm.MvNormal.dist(mu=np.zeros(len(x)), cov=cov(x[:, None])), draws=n_samples, random_seed=rng)\nzcs = (np.diff(np.sign(s), axis=1) != 0).sum(axis=1) # calculate zero crossings of each sample\n\nplt.hist(zcs, bins=np.arange(20));\nplt.xlabel(\"Number of zero crossings in 50 days\");\nplt.ylabel(\"Number of GP samples\");\nplt.xticks(np.arange(20));\nplt.title(\"Count of zero crossings, InverseGamma prior\");\n\n\n\n\n\n\n\n\nIt’s not a uniform prior, but it does center pretty nicely around 7, and the width of the distribution looks reasonable too. Three zero crossings in 50 days corresponds roughly to a zero crossing every 16 days, and 12 zero crossings in 50 days corresponds roughly to a zero crossing every 4 days. This GP should be aimed at weekly, but still be flexible enough to adapt to the data. It won’t be able to consider daily or monthly variation.\nBelow are some samples with the inverse gamma lengthscale prior. To the eye the variation for all the samples looks like it happens on weekly scales but there’s still a ton of freedom to fit different trends or patterns, as long they happen on a weekly scale.\n\n# KATE NOTE: try tweaking these and get a simple gp to git \n# Try dummy data \n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\nax.plot(x, s[np.arange(25), :].T);\n\nax.set_xlim([0, 100]);\nax.set_xlabel(\"x\");\n\n\n\n\n\n\n\n\nThe takeaway is that the best overarching principle for setting the lengthscale prior is to be as informative as possible. The point if this example is to show that if the lengthscale is allowed to float freely, GPs are really flexible, and the lengthscale doesn’t equally weight smooth and wiggly functions. In fact, it’s a bit counterintuitive, but by using a uniform prior on the lengthscale you’re actually using a very informative prior that heavily weights flat or nearly flat GPs. The best approach is to take a bit of time to come up with an informative prior for the lengthscale."
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#how-to-set-informative-priors",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#how-to-set-informative-priors",
    "title": "Intro",
    "section": "How to set informative priors",
    "text": "How to set informative priors\nMy favorite way to do this is with tail probabilities because I find them easier to think about. To me, it’s more meaningful to say, “95% of the lengthscale mass is between 1 and 5”, instead of “the mean of the lengthscale is 2, and the standard deviation is 2”. With the first statement, you’re saying that you think that it’s really unlikely that the lengthscale is smaller than 1 or larger than 5. In the second, you’re conveying a sense of where you think the lengthscale should be located, and then setting the uncertainty around that. Even if these two statements were to be equivalent, by setting tail probabilities you’re ruling values out, instead of preferring some values over others.\nYou can do this in PyMC with the utility function pm.find_constrained_prior.\n\npm.find_constrained_prior(\n    pm.Normal,\n    lower=1, upper=5, mass=0.95,\n    init_guess={\"mu\": 2, \"sigma\": 1},\n)\n\n{'mu': 2.2321082679792026, 'sigma': 0.7485883342665849}\n\n\nYou may notice it’ll fail to converge if you use too narrow of a range for inverse gammas,\n\npm.find_constrained_prior(\n    pm.InverseGamma,\n    lower=3, upper=5, mass=0.95,\n    init_guess={\"mu\": 2, \"sigma\": 1},\n)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 pm.find_constrained_prior(\n      2     pm.InverseGamma,\n      3     lower=3, upper=5, mass=0.95,\n      4     init_guess={\"mu\": 2, \"sigma\": 1},\n      5 )\n\nFile ~/miniforge3/envs/techstyle/lib/python3.11/site-packages/pymc/func_utils.py:187, in find_constrained_prior(distribution, lower, upper, init_guess, mass, fixed_params, mass_below_lower, **kwargs)\n    183 opt = optimize.minimize(\n    184     target_fn, x0=list(init_guess.values()), jac=jac, constraints=cons, **kwargs\n    185 )\n    186 if not opt.success:\n--&gt; 187     raise ValueError(\n    188         f\"Optimization of parameters failed.\\nOptimization termination details:\\n{opt}\"\n    189     )\n    191 # save optimal parameters\n    192 opt_params = {\n    193     param_name: param_value for param_name, param_value in zip(init_guess.keys(), opt.x)\n    194 }\n\nValueError: Optimization of parameters failed.\nOptimization termination details:\n message: Iteration limit reached\n success: False\n  status: 9\n     fun: 0.0014422914420487301\n       x: [ 3.961e+00  7.141e-01]\n     nit: 100\n     jac: [-1.901e-02  2.360e-02]\n    nfev: 1042\n    njev: 100\n\n\n\nWhen this happens, I’ll usually switch to pm.LogNormal. It’s a similarly shaped distribution to the inverse gamma, but it’s a bit suboptimal as a lengthscale prior because it doesn’t penalize small values as much as the inverse gamma. However, if you’re prior is so strong that the inverse gamma doesn’t work, small lengthscales won’t be a problem.\n\npm.find_constrained_prior(\n    pm.LogNormal,\n    lower=3, upper=5, mass=0.95,\n    init_guess={\"mu\": 2, \"sigma\": 0.2},\n)\n\n{'mu': 1.354045683682159, 'sigma': 0.13031529959654345}"
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#lengthscale-too-small",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#lengthscale-too-small",
    "title": "Intro",
    "section": "Lengthscale too small",
    "text": "Lengthscale too small\nOften, it’s not enought to just be more informative. There are cases where the GP will just fail to work due to how the lengthscale prior was set. We’ll start with the case when the lengthscale is too small. This case is a bit more cut and dry than the situation when the lengthscale is too large. It might be somewhat obvious, but for your case with daily x data, you wont be able to see variation that happens faster than daily (and you probably don’t care about it anyway). Here’s an example. We’ll draw samples from a GP prior, but we’ll lower the lengthscale below the resolution of the data and see what happens.\nBefore actually seeing the example, it’s helpful to know that you can think of a GP as smoothing a sample of random Gaussian noise using a kernel, just like a kernel density esimator. We’ll use that fact in the example to smooth a single sample of Gaussian noise with the set of different different lengthscales. I like doing it this way because it makes each sample directly comparable to the others.\n\nn = 50 # number of days of data\nx = np.arange(n)\n\nlengthscales = [0.5, 0.75, 1.0, 1.5, 2.0, 3.0, 5.0, 6.0]\ncolors = np.linspace(0, 1, len(lengthscales))\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\n\n# draw a sample of random noise\nz = rng.normal(size=n)\n\nfor i, ell in enumerate(lengthscales):\n    cov = pm.gp.cov.Matern52(input_dim=1, ls=ell)\n    K = cov(x[:, None]).eval()\n    L = np.linalg.cholesky(K + 1e-6 * np.eye(n))\n    \n    f = L @ z\n    \n    ax.plot(x, f, color=plt.cm.viridis(colors[i]), label=f\"$\\ell={ell}$\")\n\nm = ax.plot(x, z, color=\"k\", label=\"z\");\n\nax.legend(loc=\"upper left\");\n\nax.set_xlim([-6, n]);\nax.set_xlabel(\"day\");\nax.set_title(\"Draws from GP prior\");\n\n\n\n\n\n\n\n\nYou can see big changes in the curve at larger lengthscales, until you get down to lengthscales below a day. At \\(\\ell \\leq 1\\), they start to blur together until finally the lengthscale is small enough that no smoothing happens to z and you get z back as it was (shown as the black line).\nImagine you put a prior on \\(\\ell\\) that had all of it’s mass between zero and one. This is functionally the same as adding normal distribution prior to every day of your data – which probably isn’t what you want to do. What your prior is saying is that there is no relationship between the values of the TVP on successive days, so there’s no smoothing happening.\nA good lengthscale prior will have a pretty hard cutoff on low values of the lengthscale, especially below the resolution of the data. This is one reason that makes the inverse gamma a good probability distribution to use. It’s shown against a few other distributions below.\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\n\ns = pm.draw(pm.InverseGamma.dist(mu=2.5, sigma=1), 20_000)\nplt.hist(s, np.linspace(0, 5, 100), color=\"slateblue\", alpha=0.6, label=\"inverse gamma\");\n\ns = pm.draw(pm.Lognormal.dist(mu=0.8, sigma=0.4), 20_000)\nplt.hist(s, np.linspace(0, 5, 100), color=\"r\", alpha=0.3, label=\"log-normal\");\n\ns = pm.draw(pm.HalfNormal.dist(sigma=2), 20_000)\nplt.hist(s, np.linspace(0, 5, 100), color=\"orange\", alpha=0.2, label=\"half-normal\");\n\n\ns = pm.draw(pm.TruncatedNormal.dist(mu = 2.5, sigma=1, lower=1), 20_000)\nplt.hist(s, np.linspace(0, 5, 100), color=\"g\", alpha=0.2, label=\"truncated-normal\");\n\n\n    # x = pm.TruncatedNormal('x', mu=0, sigma=10, lower=0)\n\nax.set_yticks([]);\nax.set_xlim([0, 5]);\nax.set_xlabel(\"x\");\nax.legend();\n\n\n\n\n\n\n\n\nYou can see from the picture that for the particular values of the prior, the inverse gamma has a pretty hard cutoff for values below 1. The log-normal isn’t as strong there and the half-normal peaks at zero."
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#large-lengthscales",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#large-lengthscales",
    "title": "Intro",
    "section": "Large lengthscales",
    "text": "Large lengthscales\nA large lengthscale isn’t inherently a problem. Sometimes you want to allow large lengthscales. Two issues will likely arise though:\n\nThe GP starts to look a lot like other components that you may already have in your model, particularly the intercept. When the lengthscale is very large, the GP will model flat lines at varying heights, which is exactly what an intercept is.\nWe don’t see enough “repetitions” or cycles of the GP to be able to learn the value of lengthscale. This can cause poor sampling performance. In this case, you need to compensate by either using a very informative prior, or using a simpler component in your model, like a linear trend. It may make sense in some cases to fix the lengthscale value to a large number, especially if it’s something larger than the domain of the data.\n\nTo see what an illustration, below are 10 samples from a GP with a long lengthscale.\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\n\n## plot long lengthscale GP samples\nx = np.arange(100)\n\neta = 1\nell_long = 200\ncov = eta**2 * pm.gp.cov.Matern52(1, ls=ell_long)\n\nn_samples = 10\ns = pm.draw(pm.MvNormal.dist(mu=np.zeros(len(x)), cov=cov(x[:, None])), n_samples, random_seed=rng)\n\nax.plot(x, s.T, color=\"slateblue\", lw=3);\nax.set_xlim([0, 100]);\nax.set_xlabel(\"x\");\nax.set_title(f\"{n_samples} samples from a GP with $\\ell = {ell_long}$\");\n\n\n\n\n\n\n\n\nThese samples are all almost flat, or at best linear. Since the intercept is (usually) used to fit the mean of the data, you need to be careful if you have an intercept adding to a GP in your model. If you have two terms additive terms in your model that are doing the same thing, you’ll get identifiabilty issues. If your lengthscale is small this isn’t an issue.\nThe scale parameter, eta, also comes into play here. You’ll notice in the plot above, we set eta = 1. All the flattish GP samples are all hanging around between -1.5 and 1.5. If you were trying to fit data whose average value was 100 it would be less problematic to have a long lengthscale GP and an intercept together, because the GP and the intercept will be identifiable together.\nIt’s tricky because knowing what’s happening with the GP and how it interacts with the other parameters in your model, depends on the values of the scale eta and lengthscale ell – which change during sampling. You have to use your priors to steer things."
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#example-fitting-slowly-varying-data",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#example-fitting-slowly-varying-data",
    "title": "Intro",
    "section": "Example: fitting slowly varying data",
    "text": "Example: fitting slowly varying data\nWe’ll take one of the samples above, and use it to generate data.\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\n\nsigma_true = 0.1\nf_true = s[-1, :]\ny = f_true + sigma_true * rng.normal(size=len(f_true))\nprint(np.mean(f_true))\n\nax.plot(x, f_true);\nax.scatter(x, y, color=\"c\");\n\n0.29988853635817775\n\n\n\n\n\n\n\n\n\n\npm.find_constrained_prior(\n    pm.InverseGamma,\n    lower=50, upper=500, mass=0.95,\n    init_guess={\"mu\": 300.0, \"sigma\": 200.0},\n)\n\n{'mu': 221.26696046910808, 'sigma': 165.25644413979444}\n\n\nI’m going to do my best to put a decent prior on this from just looking at the data – pretending that we didn’t just it. I’ll walk through the reasoning I’m using. We’ll see that there are still issues sampling.\n\nThe lengthscale is definitely not smaller than say 60 or 75. I’d be suprised if it was less than 100 really. To be safe I’ll set a lower limit at 80, and say I think theres a 5% chance it’s between 50 and 500.\nThe lengthscale could be really long, it could be in the hundreds. With only a hundred days of data, we don’t see any repetitions or cycles of the function to know much at all about the lengthscale. I’ll be conservative again and say there’s a 5% chance it’s bigger than 500.\nI’m setting the value of ell in the code to be between 0.7 and 5.0, and then multiplying it by 100. pm.find_constrained_prior can sometimes have issues when the distribution has “weird” ranges for it’s support, so it helps to keep things down near zero-ish.\nThe scale eta becomes harder to estimate here too, because we don’t know if the GP will keep decreasing which will give it a huge range, or if it’s just about to head back up, making a smaller eta work. I’m going to give it an pm.Exponential prior with a 10% change that eta is greater than 3, again just to be safe. The scale eta has the same units as the y dimension of the GP. It’s analogous to the standard deviation parameter of a univariate normal distribution. This is why we always square it, setting the covariance matrix as eta**2 * pm.gp.cov.Matern52(...).\nJust looking at the data, it’s easy to guess the intercept. In more complicated models it isn’t possible to do this, so to allow for that I’ll say that it’s a normal with mean zero and standard deviation equal to five.\nFinally, the additive noise is pretty small, so I’ll use another pm.Exponential distribution with scale = 1.0. This is the median, so I’m also saying that \\(p(\\sigma &gt; 1.0) = 0.5\\) with my prior."
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#a-couple-notes",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#a-couple-notes",
    "title": "Intro",
    "section": "A COUPLE NOTES",
    "text": "A COUPLE NOTES\n\nI like to use pm.Exponential distributions for scale priors, like sigma and eta, because it’s the PC prior for a normal random effect, and I find it works well in practice. pm.HalfNormal, pm.HalfStudentT, or pm.HalfCauchy priors can work well too. If you know for sure that there’s a GP there, you can try a pm.Gamma distribution, which is like an pm.Exponential but avoids zero. This choice usually isn’t nearly as important as the lengthscale prior.\nI’m skipping ahead and using the HSGP approximation here, just for faster sampling. We’ll save how to set m and c for another discussion, because they are specific to the HSGP approximation, not GPs. You can try commenting it out and replacing it with pm.gp.Latent. The sampling will be much slower, but you don’t have to mess with m or c.\n\n\nwith pm.Model() as model:\n    # p(eta &gt; U) = alpha\n    alpha, U = 0.1, 3.0 \n    eta = pm.Exponential(\"eta\", lam=-np.log(alpha) / U)\n    \n    ell = pm.InverseGamma(\n        \"ell\",\n        **pm.find_constrained_prior(\n            pm.InverseGamma,\n            lower=50, upper=500, mass=0.95,\n            init_guess={\"mu\": 100.0, \"sigma\": 50.0},\n        )\n    )\n    cov_func = eta**2 * pm.gp.cov.Matern52(input_dim=1, ls=ell)\n    #gp = pm.gp.Latent(cov_func=cov_func) # use this instead of pm.gp.HSGP for an unapproximated GP\n    gp = pm.gp.HSGP(m=[200], c=10.0, cov_func=cov_func)\n    \n    \n    intercept = pm.Normal(\"intercept\", mu=0.0, sigma=5.0)\n    f = gp.prior(\"f\", X=x[:, None])\n\n    mu = pm.Deterministic(\"mu\", intercept + f)\n    sigma = pm.Exponential(\"sigma\", scale=1.0)\n    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n\ncmodel = nutpie.compile_pymc_model(model, backend=\"jax\", gradient_backend=\"jax\")\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\n\nsampler = nutpie.sample(\n    compiled_model=cmodel,\n    blocking=False, # allow us to make changes to the notebook while the sampler is running\n)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for 15 seconds\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    44\n                    0.08\n                    63\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    42\n                    0.07\n                    63\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    89\n                    0.10\n                    63\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    452\n                    0.11\n                    3\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    125\n                    0.12\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    385\n                    0.12\n                    31\n                \n            \n            \n        \n    \n\n\n\n\nidata = sampler.wait() # block from making changes to the notebook and wait till them sampler is finished, when its done return the idata object\n\n\nvar_names = [\n    \"eta\",\n    \"ell\",\n    \"sigma\",\n    \"intercept\",\n]\naz.summary(idata, var_names=var_names)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\neta\n0.690\n0.276\n0.289\n1.287\n0.051\n0.041\n45.0\n45.0\n1.12\n\n\nell\n95.670\n29.419\n46.995\n142.744\n4.286\n3.050\n50.0\n561.0\n1.09\n\n\nsigma\n0.094\n0.007\n0.080\n0.108\n0.001\n0.001\n47.0\n37.0\n1.08\n\n\nintercept\n-0.074\n0.535\n-0.914\n0.992\n0.129\n0.093\n19.0\n16.0\n1.26\n\n\n\n\n\n\n\n\naz.plot_trace(idata, var_names=var_names);\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(1, 3, figsize=(14, 4))\n\nmu = az.extract(idata, var_names=\"mu\").data.T\npm.gp.util.plot_gp_dist(ax=axs[0], samples=mu, x=x);\n\nf = az.extract(idata, var_names=\"f\").data.T\npm.gp.util.plot_gp_dist(ax=axs[1], samples=f, x=x);\n\nfor ax in axs[:2]:\n    ax.plot(x, f_true);\n    ax.scatter(x, y, color=\"c\");\n    ax.set_ylim([-2, 2]);\n    ax.set_xlim([0, max(x)]);\n\naz.plot_dist(idata.posterior.intercept, ax=axs[2]);\n\naxs[0].set_title(\"posterior of intercept + f\");\naxs[1].set_title(\"posterior of f only\");\naxs[2].set_title(\"posterior of intercept only\");\n\n\n\n\n\n\n\n\n\nResults\nThere are a couple observations we can make:\n\nThere are a lot of divergences. One or two aren’t a big deal , especially when dealing with GPs, but a few hundred is. Looking at the plot of posterior of intercept + f, in the left in the plot above, we can see that the data strongly informs the GP. That is, if we should be able to get a nice fit there, because we have a lot of data and relatively little noise. This information tells us to try switching to the parameterization=\"centered\". The default for HSGP is parameterization=\"noncentered\". When the GP is strongly informed by the data (think, low noise, easy to detect) the centered parameterization performs better. When the GP is buried under noise or weakly informed by the data, (think time varying parameters in MMMs, noisy data) then noncentered is usually the best). The divergences mean there’s a funnel somewhere, more information here.\nWhile the fit, intercept + mu is well informed and has a narrow posterior, the posteriors of f and the intercept are both very wide. This suggests that there is a lot of correlation in their posterior distributions. When the intercept is small, then f is large; and when the intercept is large, then f is small, always cancelling each other out. This tells us that the GP and the intercept have an identifiability issue. We can solve this either by placing a much stronger prior on the intercept (which is like adding a lot of information), or just taking that parameter out entirely.\n\nNext, we’ll switch the parameterization and remove the intercept to see if that solves the problem.\n\n\nAside: centered vs. non-centered parameterizations\nBoth parameterizations have the same number of unknown parameters, and on paper, they are mathematically equivalent. Which parameterization works better in practice (honestly, very annoyingly) depends on the geometery of the posterior distribution, how “funnely” it is. This means you have to start sampling to figure out which parameterization to use, so be ready to code it up both ways. Unfortunately no one has come up with a nice way to do this automatically in PyMC (or Stan or any other PPL). The rule of thumb is that when the posterior is strongly informed by the data, the centered parameterization works better. When the posterior is weakly informed by the data, the non-centered parameterization works better. This is a really rough argument, but I’ll try to make it for intuitions sake. Written out, the two parameterizations are:\n\nCentered: \\(x \\sim N(\\mu, \\sigma)\\).\nNon-centered \\(z \\sim N(0, 1)\\), \\(x = \\mu + z \\sigma\\).\n\nIn the centered parameterization, you’re inferring \\(x\\) directly. If you have a sharp posterior on \\(\\mu\\) and \\(\\sigma\\), it’s going to be easier to get the posterior of \\(x\\). In the non-centered parameterization, you’re instead inferring \\(z\\) and then scaling it and shifting it. The sampler has an easier time doing this when there’s more uncertainty about \\(\\mu\\) and \\(\\sigma\\).\nBelow is a super simple demo. The centered vs non-centered parameterization is commented in the model code. You could think of the following scenario: - Each observed data point is a student’s test score - Each group is a classroom within a school.\nA hierarchical model applies here because you don’t necessarily assume that each student is completely independent of the others. Since they all go to the same school, they all likely have somewhat similar socioeconomic backgrounds. This is reflected in the way the data is generated. That means there’s an global average test score across all students, and then there is a group level average test score. The value of sigma gives the student level variance, and the value of sigma_group gives the variance between he means of the groups.\nIn this example, notice that the group means are easy to detect, because sigma_group_true = 5. You’ll get a lot of divergences. Try changing to the centered parameterization. Also experiment with different values of delta_group_true, sigma_true and n_groups. Also remember to check the priors that are being set on these parameters.\n\n# simulate data from the generative process of a simple normal hierarchical model\nn_data = 100\nn_groups = 5\n\nmu_true = 1.5\nsigma_group_true = 5.0 # centered is best\n#sigma_group_true = 0.1 # non-centered is best\ndelta_group_true = sigma_group_true * rng.normal(size=n_groups)\n\nsigma_true = 1.0\nix = rng.choice(np.arange(n_groups), size=n_data)\ny_obs = mu_true + delta_group_true[ix] + sigma_true * rng.normal(size=n_data)\n\n\n# plot the generated data\nadd_jitter = lambda x, eps: x + eps * np.random.randn(len(x))\n\ngroup_labels = [\"group %i\" % i for i in range(1, n_groups + 1)]\ncolors = np.asarray([\"C%s\" % i for i in range(n_groups)])\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\n\nax.scatter(\n    add_jitter(ix, 5e-2), \n    y_obs,\n    color=colors[ix],\n    label=\"observed data\",\n);\nax.scatter(\n    np.arange(n_groups),\n    mu_true + delta_group_true,\n    marker=\"_\",\n    color=\"k\",\n    s=400,\n    label=\"group means\",\n)\n    \nax.axhline(y=mu_true, color=\"k\", lw=2, alpha=0.5, linestyle=\"--\");\nax.text(2.25, mu_true + 0.05, \"intercept\");\nax.set_xticks(np.arange(n_groups), group_labels);\n\n\n\n\n\n\n\n\n\n# PyMC model comparing centered vs. non-centered for a normal hierarchical model\n\ncoords = {\n    \"group\": group_labels,\n}\n\nwith pm.Model(coords=coords) as model:\n    ## Prior for the mean or intercept\n    mu = pm.Normal(\"mu\", sigma=5.0)\n    \n    ## PC prior for the variance of a normal random effect\n    alpha, U = 0.1, 1.0  # set tail probability, p(sigma_group &gt; U) = alpha\n    sigma_group = pm.Exponential(\"sigma_group\", lam=-np.log(alpha) / U)\n    \n    ## Centered parameterization\n    #group_effect = pm.Normal(\"group_effect\", mu=mu, sigma=sigma_group, dims=\"group\")\n    \n    ## Non-centered parameterization\n    delta_group_z = pm.Normal(\"delta_group_z\", dims=\"group\")\n    group_effect = pm.Deterministic(\"group_effect\", mu + sigma_group * delta_group_z, dims=\"group\")\n    \n    ## likelihood\n    sigma = pm.HalfNormal(\"sigma\", sigma=5.0)\n    pm.Normal(\"y\", mu=group_effect[ix], sigma=sigma, observed=y_obs)\n    \n    idata = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma_group, delta_group_z, sigma]\n/home/bill/miniforge3/envs/techstyle/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\n\n\n\n\n/home/bill/miniforge3/envs/techstyle/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 5 seconds.\nThere were 1 divergences after tuning. Increase `target_accept` or reparameterize.\n\n\nIn Gaussian processes, the centered vs. non-centered parameterization is on multivariate normals, instead of univariate normals. The different representations are: 1. Centered: \\(\\mathbf{x} \\sim \\text{MvN}(0, \\mathbf{K})\\) 2. Non-centered: \\(K = L L^T\\), \\(\\mathbf{z} \\sim \\text{MvN}(\\mu, I)\\), \\(\\mathbf{x} = \\mu + L \\mathbf{z}\\).\n\\(\\mathbf{x}\\) is the sample from the multivariate normal, \\(K\\) is the covariance matrix. \\(L\\) is the “square root” of the covariance matrix, called the Cholesky factor. Finally, \\(z\\) is a sample.\n\n\nBack to GPs\nHere’s what happens if we change the parameterization and remove the intercept.\n\nwith pm.Model() as model:\n    # tail probability p(eta &gt; 1.0) = 0.1\n    alpha, U = 0.1, 10.0\n    eta = pm.Exponential(\"eta\", lam=-np.log(alpha) / U)\n\n    ell = pm.InverseGamma(\n        \"ell\",\n        **pm.find_constrained_prior(\n            pm.InverseGamma,\n            lower=50, upper=500, mass=0.95,\n            init_guess={\"mu\": 100.0, \"sigma\": 50.0},\n        )\n    )\n    \n    cov_func = eta**2 * pm.gp.cov.Matern52(input_dim=1, ls=ell)\n    gp = pm.gp.HSGP(m=[200], c=10.0, cov_func=cov_func, parametrization=\"centered\")\n    \n    \n    #intercept = pm.Normal(\"intercept\", mu=0.0, sigma=5.0)\n    intercept = 0.0\n    f = gp.prior(\"f\", X=x[:, None])\n\n    mu = pm.Deterministic(\"mu\", intercept + f)\n    sigma = pm.Exponential(\"sigma\", scale=1.0)\n    pm.Normal(\"y\", mu=f, sigma=sigma, observed=y)\n\ncmodel = nutpie.compile_pymc_model(model, backend=\"jax\", gradient_backend=\"jax\")\n\n\nsampler = nutpie.sample(\n    compiled_model=cmodel,\n    blocking=False,\n)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 6\n    Active Chains: 0\n    \n        Finished Chains:\n        6\n    \n    Sampling for 33 seconds\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    0\n                    0.07\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    0\n                    0.06\n                    255\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    0\n                    0.13\n                    63\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    0\n                    0.05\n                    47\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    0\n                    0.10\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1300\n                    0\n                    0.06\n                    47\n                \n            \n            \n        \n    \n\n\n\n\nidata = sampler.wait()\n\n\nvar_names = [\n    \"eta\",\n    \"ell\",\n    \"sigma\",\n]\naz.summary(idata, var_names=var_names)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\neta\n0.783\n0.576\n0.166\n1.750\n0.043\n0.031\n94.0\n333.0\n1.06\n\n\nell\n90.749\n32.601\n41.341\n149.226\n6.523\n4.667\n24.0\n115.0\n1.18\n\n\nsigma\n0.093\n0.007\n0.081\n0.106\n0.000\n0.000\n1797.0\n2095.0\n1.00\n\n\n\n\n\n\n\n\naz.plot_trace(idata, var_names=var_names);\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\n\nf = az.extract(idata, var_names=\"f\").data.T\npm.gp.util.plot_gp_dist(ax=ax, samples=f, x=x);\n\nax.plot(x, f_true);\nax.scatter(x, y, color=\"c\");\nax.set_ylim([-0.5, 1.0]);\nax.set_xlim([0, np.max(x)])\nax.set_title(\"posterior of f\");\n\n\n\n\n\n\n\n\nIt’s not perfect but it’s much better. There’s no intercept so we know the identifiability issue is gone. We’re still getting warnings about r_hat though. Since the function changes so slowly, the model is unable to learn much about the lengthscale, only that it’s large. With data like this, a GP is probably overkill. We might be better off just using a simple linear trend to model something like this, but that depends on the context. Below we’ll plot the posterior vs. the prior on the lengthscale.\n\nell = az.extract(idata, var_names=\"ell\").data\n\nbins = np.linspace(0, 350, 100)\nplt.hist(ell, density=True, bins=bins, label=\"posterior\");\n\nell_prior = pm.InverseGamma.dist(\n    **pm.find_constrained_prior(\n        pm.InverseGamma,\n        lower=50.0, upper=500.0, mass=0.95,\n        init_guess={\"mu\": 100.0, \"sigma\": 50.0},\n    )\n)\ns = pm.draw(ell_prior, 6000)\nplt.hist(s, density=True, bins=bins, alpha=0.5, label=\"prior\", color=\"k\");\n\nplt.yticks([]);\nplt.title(\"Lengthscale prior and posterior\");\nplt.legend();\n\n\n\n\n\n\n\n\nYou can see that we didn’t learn too much about the lengthscale from the data. This is a pretty common pattern with GPs, the data doesn’t inform the lengthscale well. This isn’t necessarily bad though – what’s really happeneing is we’re averaging over many possible models with varying complexity, just using a single lengthscale parameter. For more info on lengthscales, take a look at this short tech note.\nYou could also consider fixing the lengthscale to a large value, far above the range of the data. I’d put a heavy asterisk on the results of a model with a fixed lengthscale though, especially in the context of forecasting, because you can also understand the lengthscale as controlling how far into the future you can make forecasts. This is something you want the data to determine. We’ll see this later.\nTo summarize, the first problem was using the noncentered parameterization when the GP is strongly informed by the data. Then the second issue was that intercept and the GP are difficult to identify in this case. If you shifted the GP up so it was centered around say, y=100, then you would want to keep the intercept!"
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#example-2-a-more-common-example",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#example-2-a-more-common-example",
    "title": "Intro",
    "section": "Example 2: A more common example",
    "text": "Example 2: A more common example\nThis problem was difficult, and we had to dig into the bag of tricks a bit to get it to fit reasonably well. Now let’s see what happens on a less unusual problem. Below is a squared sin wave that’s oscillating on a monthly time scale. It represents the average number of sales per day. The observed data is a sales count, so we’ll model this as a Poisson process. This may be more like problems you’ll encounter in practice. We have some zero crossings so we’ll be able to say something more concrete about the best lengthscale.\nThe Poisson distribution is a simple distribution that over integer values, 0, 1, 2, 3, etc. It’s often used for count data. It’s even simpler than the normal because it’s controlled by one parameter (mu) instead of two for the normal, mu and sigma. This is why I chose it for the example. In a Poisson distribution, mu is both the mean and the variance, and it often makes sense that as the mean increases the variance also increases.\nYou need to exponentiate the linear predictor (you’d say you’re using the log link function) because the mean of a Poisson is restricted to be positive.\nI had to restart the kernel, and not use the nutpie sampler to get the prediction examples to work. I’m sure this’ll get fixed soon.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport pytensor.tensor as pt\nimport arviz as az\nimport nutpie\n\nRANDOM_SEED = 8998\nrng = np.random.default_rng(RANDOM_SEED)\n\n\nx = np.arange(70)\nf_true = np.exp(2.0 * np.sin(2 * np.pi * x * (1 / 30)))\n\ny = rng.poisson(lam=f_true)\n\nplt.plot(x, f_true);\nplt.scatter(x, y, color=\"c\");\nplt.xlabel(\"day\");\nplt.ylabel(\"number of sales\");\n\n\n\n\n\n\n\n\nAgain I’ll lay out my thought process for setting priors. You’ll notice most of it is the same as before, exponential prior on eta, inverse gamma on ell. This time though the likelihood is Poisson so we don’t have a sigma parameter to worry about, but we do need to exponentiate the GP to keep it positive valued.\n\nI’ll set eta first. The data ranges from around 0 to 12. The log of 12 is about 2.5, so eta is going to be somewhere around there. I’m going to say that there’s a 10% chance eta is larger than 5.0 to be conservative.\nWe know the process repeats every 30 days. But, one thing about lengthscales I haven’t mentioned is that they need to be small enough to handle the most wiggly significant part of the curve. A short lengthscale can follow along a slowly changing curve, but a long lengthcale GP can’t bend enough to hit a quick variation. Those peaks are pretty sharp, so I’ll say that the lengthscale should be between 5 and 35 with a 95% probability.\nSince we’re using the HSGP again, I’ll use the function pm.gp.hsgp_approx.approx_hsgp_hyperparams to set help me choose m and c.\n\n\n## calculate minimum recommended m and c values (larger is ok, but less efficient)\nm, c = pm.gp.hsgp_approx.approx_hsgp_hyperparams(x_range=[0, 200], lengthscale_range=[3, 20], cov_func=\"matern52\")\nm, c\n\n(106, 1.2)\n\n\n\nwith pm.Model() as model:\n    alpha, U = 0.1, 5.0\n    eta = pm.Exponential(\"eta\", lam=-np.log(alpha) / U)\n\n    ell = pm.InverseGamma(\n        \"ell\",\n        **pm.find_constrained_prior(\n            pm.InverseGamma,\n            lower=3.0, upper=20.0, mass=0.95,\n            init_guess={\"mu\": 10.0, \"sigma\": 5.0},\n        )\n    )\n    \n    cov_func = eta**2 * pm.gp.cov.Matern52(input_dim=1, ls=ell)\n    gp = pm.gp.HSGP(m=[200], c=3.0, cov_func=cov_func, parametrization=\"centered\")\n\n    X = pm.Data(\"X\", x[:, None])\n    phi, sqrt_psd = gp.prior_linearized(X=X)\n    basis_coeffs = pm.Normal(\"basis_coeffs\", size=gp.n_basis_vectors)\n    f = pm.Deterministic(\"f\", phi @ (basis_coeffs * sqrt_psd))\n    \n    log_mu = pm.Deterministic(\"log_mu\", f)\n    pm.Poisson(\"y\", mu=pt.exp(log_mu), observed=y)\n\nWe’re using an alternative way to write the GP to make it easier to make forecasts. Instead of gp.prior, we’re using gp.prior_linearized. pm.gp.HSGP is meant to be a drop in replacement to pm.gp.Latent with minimal changes to the code. However, one really big benefit of the HSGP approximation is that under the hood it’s a linear model, which means we can use pm.set_data instead of having to write custom GP specific code for predictions. Compare how you make predictions for pm.gp.Latent here vs. how you make predictions for pm.gp.HSGP here.\n\nwith model:\n    idata = pm.sample(nuts_sampler=\"numpyro\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere were 6 divergences after tuning. Increase `target_accept` or reparameterize.\n\n\n\nvar_names = [\n    \"eta\",\n    \"ell\",\n]\naz.summary(idata, var_names=var_names)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\neta\n2.090\n0.703\n1.008\n3.382\n0.022\n0.016\n1172.0\n1197.0\n1.0\n\n\nell\n8.828\n2.186\n5.000\n12.827\n0.060\n0.044\n1465.0\n1469.0\n1.0\n\n\n\n\n\n\n\n\naz.plot_trace(idata, var_names=var_names);\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\n\nf = az.extract(idata, var_names=\"f\").data.T\npm.gp.util.plot_gp_dist(ax=ax, samples=np.exp(f), x=x);\n\nax.plot(x, f_true);\nax.scatter(x, y, color=\"c\");\nax.set_xlim([0, np.max(x)])\nax.set_title(\"posterior of f\");\n\n\n\n\n\n\n\n\nThere were a few divergences. I tried it with parametrization=non-centered and that didn’t really help, so I think it’s safe to increase the target_accept of the sampler and try again. The reason you don’t just start by setting target_accept=0.999 and forget about it is that it’ll both slow down sampling and potentially paper over issues with your model that could be more robustly fixed by reparameterization of searching for identifiability issues. Also, if there are serious identifiability / funnel issues, it’s unlikely that increasing target_accept will be enough to fix sampling.\n\nwith model:\n    idata = pm.sample(nuts_sampler=\"numpyro\", target_accept=0.95)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n\n\n\nvar_names = [\n    \"eta\",\n    \"ell\",\n]\naz.summary(idata, var_names=var_names)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\neta\n2.077\n0.726\n0.999\n3.396\n0.016\n0.011\n2054.0\n2626.0\n1.0\n\n\nell\n8.757\n2.151\n4.852\n12.623\n0.039\n0.029\n3212.0\n3229.0\n1.0\n\n\n\n\n\n\n\n\naz.plot_trace(idata, var_names=var_names);"
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#example-3-a-little-less-data",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#example-3-a-little-less-data",
    "title": "Intro",
    "section": "Example 3: a little less data",
    "text": "Example 3: a little less data\n\nx = np.arange(64)\nperiod = 30\nf_true = np.exp(2.0 * np.sin(2 * np.pi * x * (1 / period)))\n\ny = rng.poisson(lam=f_true)\n\nplt.plot(x, f_true);\nplt.scatter(x, y, color=\"c\");\nplt.xlabel(\"day\");\nplt.ylabel(\"number of sales\");\n\n\n\n\n\n\n\n\n\nwith pm.Model() as model:\n    alpha, U = 0.1, 5.0\n    eta = pm.Exponential(\"eta\", lam=-np.log(alpha) / U)\n\n    ell = pm.InverseGamma(\n        \"ell\",\n        **pm.find_constrained_prior(\n            pm.InverseGamma,\n            lower=3.0, upper=20.0, mass=0.95,\n            init_guess={\"mu\": 10.0, \"sigma\": 5.0},\n        )\n    )\n    \n    cov_func = eta**2 * pm.gp.cov.Matern52(input_dim=1, ls=ell)\n    gp = pm.gp.HSGP(m=[200], c=3.0, cov_func=cov_func, parametrization=\"centered\")\n\n    X = pm.Data(\"X\", x[:, None])\n    phi, sqrt_psd = gp.prior_linearized(X=X)\n    basis_coeffs = pm.Normal(\"basis_coeffs\", size=gp.n_basis_vectors)\n    f = pm.Deterministic(\"f\", phi @ (basis_coeffs * sqrt_psd))\n    \n    log_mu = pm.Deterministic(\"log_mu\", f)\n    pm.Poisson(\"y\", mu=pt.exp(log_mu), observed=y)\n\n\nwith model:\n    idata = pm.sample(nuts_sampler=\"numpyro\", target_accept=0.95)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx_new = np.arange(100)\n\nwith model:\n    pm.set_data({\"X\": x_new[:, None]})\n\n    idata.extend(pm.sample_posterior_predictive(idata, var_names=[\"log_mu\"]))\n\nSampling: []\n\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\n\nlog_mu = az.extract(idata.posterior_predictive, var_names=\"log_mu\").T\npm.gp.util.plot_gp_dist(ax=ax, samples=np.exp(log_mu), x=x_new);\n\nax.plot(x, f_true);\nax.scatter(x, y, color=\"c\");\nax.set_xlim([0, np.max(x_new)])\nax.set_ylim([0, 20]);\nax.set_title(\"posterior of f\");\n\n\n\n\n\n\n\n\nThis forecast looks a lot different, but it’s doing the same thing. Continuing the trend forward and then reverting to the prior after about a lengthscale number of days. What if we force the lengthscale to be smaller?\nWe can go into the model and fix the lengthscale parameter to a specific value with pm.do:\n\nwith pm.do(model, {\"ell\": 2, \"eta\": 1.5}) as m2:\n    \n    idata2 = pm.sample_posterior_predictive(idata, var_names=[\"log_mu\"])\n\nSampling: []\n\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\n\nlog_mu = az.extract(idata2.posterior_predictive, var_names=\"log_mu\").T\npm.gp.util.plot_gp_dist(ax=ax, samples=np.exp(log_mu), x=x_new);\n\nax.plot(x, f_true);\nax.scatter(x, y, color=\"c\");\nax.set_xlim([0, np.max(x_new)])\nax.set_ylim([0, 20]);\nax.set_title(\"posterior of f, lengthscale fixed at 2\");\n\n\n\n\n\n\n\n\nYou’ll notice that to get reasonable results you have to adjust eta as well – try different values. This is because for GPs, the posterior of eta and ell is always correlated. To get the same fit with a smaller lengthscale, you need to decrease the scale, and visa versa. Feel free to experiment with different values to get a feel for it. You’ll notice in the above plot though that the mean reversion happens extremely quickly, I’d say from \\(x=64\\) to \\(x=70\\). By \\(x=80\\) we are back at the prior. You’ll also see the effect of the lengthscale on the wigglyness of the posterior samples (faint red lines).\n\nwith pm.do(model, {\"ell\": 16, \"eta\": 3.5}) as m3:\n    \n    idata3 = pm.sample_posterior_predictive(idata, var_names=[\"log_mu\"])\n\nSampling: []\n\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 5))\n\nlog_mu = az.extract(idata3.posterior_predictive, var_names=\"log_mu\").T\npm.gp.util.plot_gp_dist(ax=ax, samples=np.exp(log_mu), x=x_new);\n\nax.plot(x, f_true);\nax.scatter(x, y, color=\"c\");\nax.set_xlim([0, np.max(x_new)])\nax.set_ylim([0, 20]);\nax.set_title(\"posterior of f, lengthscale fixed at 16\");\n\n\n\n\n\n\n\n\nHere the posterior samples are much smoother because of the long lengthscale, and the reversion to the mean happens much more slowly, well past \\(x=80\\).\nTo get “smarter” predictions from GPs, we need to find a way to either learn: 1. Longer lengthscales 2. Periodicity"
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#setting-the-lengthscale-prior-for-a-periodic-gp",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#setting-the-lengthscale-prior-for-a-periodic-gp",
    "title": "Intro",
    "section": "Setting the lengthscale prior for a periodic GP",
    "text": "Setting the lengthscale prior for a periodic GP\nThe lengthscale controls how flexible the GP is within a period. I don’t know of any papers or recommendations out there for how to choose it in the context of periodic GPs. The values that work end up being a lot smaller than I would expect, so I’m not sure on the precise interpretation. I think it’s less important than for Matern family GPs because it’s in a periodic GP the period is the more important quantity. Note that you can set a prior on the period and not use a fixed value, but the sampler will have a hard time and you likely do know the period in most contexts.\nSince you’re telling the GP that it’s looking at many repetition of the same pattern, the lengthscale is usually quite a bit easier to learn. Just remember when setting it that it’s going to be a much smaller value than you’d expect, and be sure to check the prior predictive distribution of the GP samples at a few different values of the lengthscale, like the one’s we’ll look at below.\n\nPrior samples where the lengthscale is drawn from a prior\n\nx = np.arange(64)\n\nwith pm.Model() as model:\n    alpha, U = 0.1, 5.0\n    eta = pm.Exponential(\"eta\", lam=-np.log(alpha) / U)\n\n    ell = pm.Lognormal(\n        \"ell\",\n        **pm.find_constrained_prior(\n            pm.Lognormal,\n            lower=0.5, upper=5.0, mass=0.95,\n            init_guess={\"mu\": 5.0, \"sigma\": 1.0},\n        )\n    )\n\n    period = 30.0\n    cov_func = pm.gp.cov.Periodic(input_dim=1, period=period, ls=ell)\n    gp = pm.gp.HSGPPeriodic(m=50, scale=eta, cov_func=cov_func)\n    \n    X = pm.Data(\"X\", x[:, None])\n    (phi_cos, phi_sin), psd = gp.prior_linearized(X=X)\n\n    m = gp._m\n    beta = pm.Normal(\"beta\", size=(m * 2 - 1))\n\n    # The (non-centered) GP approximation is given by\n    f = pm.Deterministic(\n        \"f\",\n        phi_cos @ (psd * beta[:m]) + phi_sin[..., 1:] @ (psd[1:] * beta[m:])\n    )\n    \n    idata = pm.sample_prior_predictive()\n\n\nf = az.extract(idata.prior, var_names=\"f\").data\nplt.plot(f[:, :20]);\n\nSampling: [beta, ell, eta]\n\n\n\n\n\n\n\n\n\n\n\nPrior samples with a small lengthscale\n\nwith pm.do(model, {\"ell\": 0.5}) as model:\n    idata = pm.sample_prior_predictive()\n    f = az.extract(idata.prior, var_names=\"f\").data\n    plt.plot(f[:, :5]);\n\nSampling: [beta, eta]\n\n\n\n\n\n\n\n\n\n\n\nPrior samples with a large lengthscale\n\nwith pm.do(model, {\"ell\":  3}) as model:\n    idata = pm.sample_prior_predictive()\n    f = az.extract(idata.prior, var_names=\"f\").data\n    plt.plot(f[:, :5]);\n\nSampling: [beta, eta]\n\n\n\n\n\n\n\n\n\nLarger lengthscales are much more “sinusoidal” so there’s no variation within a cycle, and smaller lengthscales vary much more. Overall, I’d recommend not necessarily following the “inverse gamma is good” rule for lengthscales on periodic GPs. Truncated normals (lengthscales still must be positive) or gamma distributions might work better.\nYou’ll notice the intercept problem is much more pronounced with periodic GPs, particularly for larger lengthscales. I think having a heavy right tail on the prior in this situation would be harmful.\n\n\nWhy do Periodic GPs involve more of a level shift?\nYou can see why in this plot from earlier, which compared the covariance as a function of distance.\n\nx = np.linspace(0, 10, 100)\n\ncov1 = pm.gp.cov.Matern52(1, ls=3)\ncov2 = pm.gp.cov.Periodic(1, period=3, ls=0.5)\n\nK1 = cov1(x[:, None])\nK2 = cov2(x[:, None])\n\nplt.plot(x, K1[:, 0].eval(), label=\"Matern52\")\nplt.plot(x, K2[:, 0].eval(), label=\"Periodic\");\nplt.legend();\nplt.ylabel(\"covariance\");\nplt.xlabel(\"x distance\");\n\n\n\n\n\n\n\n\nNotice that the covariance never goes to zero for the Periodic example. That means that all points in the GP have a non-negligible covariance with all the other points. This is equivalent to an intercept. It’s kind of strange at first but it does make sense. You can see this by drawing samples from a multivariate normal who’s covariance matrix is just a square matrix of all ones.\n\nK = np.ones((100,100))\ns = pm.draw(pm.MvNormal.dist(mu=np.zeros(100), cov=K), 5).T\n\nplt.plot(s);\n\n\n\n\n\n\n\n\nTo see how this shows up for the periodic covariance matrix, we’ll remake an earlier figure but with different values of the covariance. This plot shows the covariance as a function the distance between two x values, x and x’. At small lengthscales the covariance goes to zero at some distances. As the lenthscale increases, the covariance stays positive everywhere. When there’s a non-negligible covariance everywhere, that allows the level of the GP to shift up and down. It’s something to be mindful of when setting priors on the GP lengthscales, and on whether you should include an intercept parameter with the GP.\n\nx = np.linspace(0, 10, 100)\n\ncov0 = pm.gp.cov.Periodic(1, period=3, ls=0.2)\ncov1 = pm.gp.cov.Periodic(1, period=3, ls=0.3)\ncov2 = pm.gp.cov.Periodic(1, period=3, ls=0.4)\ncov3 = pm.gp.cov.Periodic(1, period=3, ls=0.5)\n\nK0 = cov0(x[:, None])\nK1 = cov1(x[:, None])\nK2 = cov2(x[:, None])\nK3 = cov3(x[:, None])\n\nplt.plot(x, K0[:, 0].eval(), label=\"ls=0.2\")\nplt.plot(x, K1[:, 0].eval(), label=\"ls=0.3\")\nplt.plot(x, K2[:, 0].eval(), label=\"ls=0.4\");\nplt.plot(x, K3[:, 0].eval(), label=\"ls=0.5\");\nplt.legend();\nplt.ylabel(\"covariance\");\nplt.xlabel(\"x distance\");\nplt.axhline(y=0, color=\"k\", linestyle=\":\", alpha=0.5);\n\n\n\n\n\n\n\n\nBut, there is a trick we can use to enforce the periodic GP to have zero mean. It’s no longer mathematically GP with a periodic covariance function, it’s something else that’s similar but not exactly the same."
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#zero-mean-constraints-periodic-gp",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#zero-mean-constraints-periodic-gp",
    "title": "Intro",
    "section": "Zero mean constraints, periodic GP",
    "text": "Zero mean constraints, periodic GP\nWe’ll start with the zero mean constraint for the periodic GP, since this is likely to be the most useful to you. To enforce the constraint, simply subtract the mean from each of the sine and cosine basis vectors.\n\nx = np.arange(64)\n\nwith pm.Model() as model:\n    alpha, U = 0.1, 5.0\n    eta = pm.Exponential(\"eta\", lam=-np.log(alpha) / U)\n\n    ell = pm.Lognormal(\n        \"ell\",\n        **pm.find_constrained_prior(\n            pm.Lognormal,\n            lower=0.5, upper=5.0, mass=0.95,\n            init_guess={\"mu\": 5.0, \"sigma\": 1.0},\n        )\n    )\n\n    period = 30.0\n    cov_func = pm.gp.cov.Periodic(input_dim=1, period=period, ls=ell)\n    gp = pm.gp.HSGPPeriodic(m=50, scale=eta, cov_func=cov_func)\n    \n    X = pm.Data(\"X\", x[:, None])\n    (phi_cos, phi_sin), psd = gp.prior_linearized(X=X)\n\n    m = gp._m\n    beta = pm.Normal(\"beta\", size=(m * 2 - 1))\n\n    # subtract the mean from each of the basis vectors\n    phi_cos = phi_cos - pt.mean(phi_cos, axis=0)\n    phi_sin = phi_sin - pt.mean(phi_sin, axis=0)\n    \n    f = pm.Deterministic(\n        \"f\",\n        phi_cos @ (psd * beta[:m]) + phi_sin[..., 1:] @ (psd[1:] * beta[m:])\n    )\n    \n    idata = pm.sample_prior_predictive()\n\n\nf = az.extract(idata.prior, var_names=\"f\").data\nplt.plot(f[:, :20]);\n\nSampling: [beta, ell, eta]\n\n\n\n\n\n\n\n\n\nNow you can see that all the samples from the GP hover around zero much better. If you calculate the mean of each sample you’ll see they are all nearly exactly zero. I think this is worth trying whenever you need to use a Periodic HSGP, and maybe it should be part of the PyMC code…"
  },
  {
    "objectID": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#zero-mean-constraints-regular-hsgps",
    "href": "posts/how-to-fit-a-gp/11-03-2024_how-to-fit-a-GP.html#zero-mean-constraints-regular-hsgps",
    "title": "Intro",
    "section": "Zero mean constraints, regular HSGPs",
    "text": "Zero mean constraints, regular HSGPs\nYou can use the exact same trick for regular HSGPs too, subtract the mean from each of the basis vectors. To do so you’ll have to write a little custom code, but overall it’s pretty straightforward to do. The place to start is by reading the source for HSGP.prior_linearized. I’ll skip doing that here because I suspect it’ll be less useful for your usecase. If you enforce the HSGP to have exactly zero mean, you won’t be able to use it to forecast. If you don’t need to forecast into the future, and you still do need the constraint, I’d suggest switching to splines. The patsy documentation (though it’s not super explicit) shows how to apply zero mean, monotonic increasing and decreasing, and periodic constraints to spline functions.\nA super fast trick I sometimes use to apply a zero mean constraint (though it doesn’t always work), is to use a Potential to add a “soft” zero mean constraint. The code to force a variable x to have zero mean is:\npm.Potential(\"constraint\", pm.logp(pm.Normal.dist(mu=0.0, sigma=0.001), pt.mean(x)))\nTranslated into english, what this says is:\n“Add a term to the log likelihood that puts a normal prior, with mean zero and small sigma, on the mean of x”. This penalizes deviations of the mean of x away from zero. This can be a bit touchy where you may have to fidle with the value of sigma, but this trick can be useful sometimes, mostly because it’s so easy to apply and you can apply it to any vector without too much thought."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Intro\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bias-Variance tradeoff vs. ‘Occam’s Razor’\n\n\n\n\n\n\nbayesian\n\n\n\n\n\n\n\n\n\nMar 3, 2025\n\n\nBill Engels\n\n\n\n\n\n\nNo matching items"
  }
]