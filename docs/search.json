[
  {
    "objectID": "posts/occams-razor/occams-razor.html",
    "href": "posts/occams-razor/occams-razor.html",
    "title": "Putting priors on model complexity",
    "section": "",
    "text": "import pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nseed = 12345678\nrng = np.random.default_rng(seed)\nThere’s an example I really like that comes from this short little paper titled “Occam’s razor” from I think NIPS 2000. I originally came across a version of it in the first chapter of Andrew G. Wilson’s thesis. It’s what made it click for me why non-parametric model components like GPs often work better than parametric models like polynomials, splines or other basis function methods.\nIt builds on the example you’ve probably seen in textbooks that demonstrates the bias variance tradeoff by fitting polynomials of increasing degree to some dataset. Or, more closely, using the Bayesian evidence to select the right polynomial degree.\nfrom IPython.display import Image\nfrom IPython.core.display import HTML \nImage(url= \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Bias_and_variance_contributing_to_total_error.svg/2560px-Bias_and_variance_contributing_to_total_error.svg.png\")\nI saw this quote in Gelman’s blog recently that reminded me of it. The quote, from Radford Neal, is:\nThe goal of this post is to illustrate with a toy problem why this is good advice.\nLet’s frame the problem in terms of trying to fit some polynomial curve to observed data. To rephrase the quote in practical terms, when you’re trying to model some unknown function, finding the “right” number of basis functions is a task were you’re deliberately limiting the complexity of the model. The real goal is to get a parsimonious representation of the unknown function – no one is asking you directly about the best number of basis vectors you ended up using to do that. It would be better instead to use a complex model of which the simple model is a special case, and let the data decide. At the risk of simplifying too much, the reason people are interested in Gaussian processes is because they are really good at doing this automatically."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#the-two-experiments",
    "href": "posts/occams-razor/occams-razor.html#the-two-experiments",
    "title": "Putting priors on model complexity",
    "section": "The two experiments",
    "text": "The two experiments\nWe’re going to run two experiments. First, and you’ve probably seen before, we’re going to build a sequence of polynomial regression models with increasing complexity and then choose the “best” model, according to the leave-one-out crossvalidation (LOO-CV) metric. We’ll use the exact same priors for the polynomial coefficients.\nThen in the second experiment, we’ll do the exact same thing, except, we’ll keep decreasing the prior scale we use for the polynomial coefficients as we add more terms. We’re effectively using our priors as a hedge against overfitting. Then we’ll see what LOO-CV says is the best model, and compare the results to the first experiment.\nFor our toy example, here’s some data that’s generated from a step function with a bit of additive Gaussian noise.\n\nx = np.linspace(0, 2, 50)\nf = np.ones(50)\nf[:25] = 0.0\ny = f + 0.3 * rng.normal(size=len(x))\n\nplt.plot(x[:25], f[:25], color='k', label=\"underlying function\")\nplt.plot(x[25:], f[25:], color='k')\nplt.plot(x, y, 'o', label=\"observed data\");\nplt.xlim([0, 2]);\nplt.xlabel(\"x\");\nplt.ylabel(\"y\");\nplt.legend(loc=\"lower right\");\nplt.title(\"Our toy dataset\");\n\n\n\n\n\n\n\n\n\nDetails\nBefore getting going and actually modeling this, there’s one preliminary thing to mention. Feel free to skip over this bit about the QR decomposition because it’s not really core to the goal here. We just need it to make our priors on the polynomial coefficients comparable and on the same overall scale.\nWe’ll use the QR decomposition to orthogonalize our basis. I found some Python code to do this here, pasted below.\n\ndef ortho_poly_fit(x, degree = 1):\n    n = degree + 1\n    x = np.asarray(x).flatten()\n    if(degree &gt;= len(np.unique(x))):\n            stop(\"'degree' must be less than number of unique points\")\n    xbar = np.mean(x)\n    x = x - xbar\n    X = np.vander(x, n, increasing=True)\n    q,r = np.linalg.qr(X)\n\n    z = np.diag(np.diag(r))\n    raw = np.dot(q, z)\n\n    norm2 = np.sum(raw**2, axis=0)\n    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]\n    Z = raw / np.sqrt(norm2)\n    return Z, norm2, alpha\n\nThe left panel below shows the first four polynomial basis vectors and gives their correlation matrix. The right panel shows the same four basis vectors after they’ve been orthogonalized.\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\ndegree = 3\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10.0, 4.0])\n\n## SUBPLOT 1\nX = np.vander(x, degree + 1, increasing=True)[:, :degree+1]\nax1.plot(x, X, lw=2);\nax1.set_xlim([0.0, 2.0])\nax1.set_ylim([-0.2, 8.0])\n\naxins1 = inset_axes(ax1, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nXcorr = np.corrcoef(X[:, 1:].T)\naxins1.set_xticks([])\naxins1.set_yticks([])\naxins1.imshow(Xcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Xcorr[j, i]\n        axins1.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"white\")\n\n## SUBPLOT 2\nZ, _, _ = ortho_poly_fit(x, degree=degree)\nax2.plot(x, Z, lw=2);\nax2.set_xlim([0.0, 2.0])\nax2.set_ylim([-0.5, 0.9])\n\naxins2 = inset_axes(ax2, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nZcorr = np.corrcoef(Z[:, 1:].T)\naxins2.set_xticks([])\naxins2.set_yticks([])\naxins2.imshow(Zcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Zcorr[j, i]\n        axins2.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"c\")\n\n\n\n\n\n\n\n\nThere are other benefits to orthogonalizing the basis, but our main reason is to have all the basis function coefficients living on the same scale. Notice in the left panel the basis functions range from [0, 2], [0, 4] and [0, 9], so scale-wise they’re all over the place. A lesser reason (for our purposes here) is that the basis functions are now orthogonal, which makes things quite a bit easier on the sampler. The two inset panels show the correlation matrix of the 3 non-constant basis vectors."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#bonus-points",
    "href": "posts/occams-razor/occams-razor.html#bonus-points",
    "title": "Putting priors on model complexity",
    "section": "Bonus points",
    "text": "Bonus points\nWhat about gamma, how did we choose that? The Occam’s razor paper also talks about this (at the start of Sec. 3). This time we’ll just run it once as a 30 degree polynomial (which is only 5 less than the total number of data points!). We should get the same ELPD. Yes, this is how I chose the magic number gamma = 1.25 above.\n\nwith pm.Model() as model:\n    degree = 30\n    Z, _, _ = ortho_poly_fit(x, degree=degree)\n   \n    gamma = pm.Exponential(\"gamma\", scale=1.0)\n    scale = 5.0 * np.arange(1, degree + 2) ** (-gamma)\n    beta = pm.Normal(\"beta\", mu=0.0, sigma=scale, size=degree + 1)\n   \n    mu = pm.Deterministic(\"mu\", Z @ beta)\n    sigma = pm.Exponential(\"sigma\", scale=5.0)\n    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n        \n    idata = pm.sample(nuts_sampler=\"nutpie\", target_accept=0.9)\n    pm.sample_posterior_predictive(idata, progressbar=False, extend_inferencedata=True)\n    pm.compute_log_likelihood(idata, progressbar=False)\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00  Chains in warmup: 0, Divergences: 0]\n    \n    \n\n\nSampling: [y]\n\n\n\naz.plot_trace(idata, var_names=\"gamma\");\n\n\n\n\n\n\n\n\n\nelpd_data = az.loo(idata)\nprint(f\"ELPD: {elpd_data.elpd_loo:0.2f}, ELPD SE: {elpd_data.se:0.2f}\")\n\nELPD: -14.71, ELPD SE: 5.88\n\n\n\nall_idatas = {**idata_results1, **idata_results2, **{\"Prior on gamma\": idata}}\n\n\naz.compare(all_idatas).head(10)\n\n/Users/andre/miniconda3/envs/dev/lib/python3.11/site-packages/arviz/stats/stats.py:309: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'True' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n/Users/andre/miniconda3/envs/dev/lib/python3.11/site-packages/arviz/stats/stats.py:309: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'log' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nPrior on gamma\n0\n-14.713950\n11.646827\n0.000000\n0.614029\n5.884088\n0.000000\nTrue\nlog\n\n\nExp 2, degree 7\n1\n-17.903478\n7.316925\n3.189529\n0.272241\n5.682735\n8.401857\nFalse\nlog\n\n\nExp 1, degree 7\n2\n-17.990226\n8.009275\n3.276277\n0.113730\n5.163729\n8.268976\nTrue\nlog\n\n\nExp 2, degree 9\n3\n-18.163848\n7.680631\n3.449898\n0.000000\n5.568024\n8.282049\nFalse\nlog\n\n\nExp 2, degree 8\n4\n-18.203050\n7.603282\n3.489101\n0.000000\n5.602184\n8.322322\nTrue\nlog\n\n\nExp 2, degree 12\n5\n-18.206949\n8.157549\n3.493000\n0.000000\n5.197726\n8.023377\nTrue\nlog\n\n\nExp 2, degree 11\n6\n-18.276150\n8.165427\n3.562200\n0.000000\n5.300492\n8.106990\nTrue\nlog\n\n\nExp 2, degree 10\n7\n-18.485222\n8.217779\n3.771272\n0.000000\n5.498343\n8.270510\nTrue\nlog\n\n\nExp 2, degree 27\n8\n-18.530088\n10.165056\n3.816139\n0.000000\n5.392681\n8.231695\nFalse\nlog\n\n\nExp 2, degree 19\n9\n-18.534833\n9.554110\n3.820883\n0.000000\n5.349856\n8.187055\nFalse\nlog\n\n\n\n\n\n\n\n\nThis shows that putting a prior on gamma actually gives the best model, by far."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#summary",
    "href": "posts/occams-razor/occams-razor.html#summary",
    "title": "Putting priors on model complexity",
    "section": "Summary",
    "text": "Summary\nWe’ve essentially transferred our “prior” on model complexity from a discrete problem via the choice of the number of basis vectors, to a non-scare-quoted continuous prior on model complexity, represented by the gamma parameter. This example nicely demonstrates that the common example you’ve probably seen about model complexity and the bias variance trade-off isn’t really the whole story. I hope this gives a concrete example of the quote\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nI think it’s kind of interesting that Bayesian regression with normal priors on the coefficients is equivalent to ridge regression, where the scale you use on all the priors is the L2 penalty \\(\\lambda\\). Does it make sense to do ridge regression where successive coefficients get penalized more and more by increasing \\(\\lambda\\)?\nTheres also a direct conceptual connection to Gaussian processes, which can be seen as models that use an infinite number of basis vectors (explained in chapter 2 of R+W’s GPML book). Those infinite number of basis numbers have very particular priors though, which for Matern family GPs, model complexity is directly controlled by the lengthscale parameter.\nThere’s also a relationship to penalized splines. There’s a nice case study in Stan and an implementation here in PyMC. The same underlying concept is at play here too. The idea is not to spend too much time choosing the number and location of knots, but instead choose “too many” knots and rely on a smoothing prior on the coefficients to control model complexity.\nFor practical problems, it means you don’t need to do model selection for these types of problems, you just need a model that can handle the complexity of the data and carefully choose your priors."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nPutting priors on model complexity\n\n\n\n\n\n\nbayes\n\n\ncode\n\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\nBill Engels\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What about this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]