[
  {
    "objectID": "occams-razor.html",
    "href": "occams-razor.html",
    "title": "Putting priors on model complexity, fix title not about PC priors",
    "section": "",
    "text": "import pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nseed = 12345678\nrng = np.random.default_rng(seed)\nI saw this quote in Gelman’s blog recently that reminded me of a really cool example I saw, from this short paper titled “Occam’s razor” from NIPS 2000. This example is what made it clock for me why non-parametric model components like Gaussian processes often work better than parametric models like polynomials, splines, or other basis function methods. The quote (from Radford Neal), is:\nThe goal of this post is to reproduce that example in a modern context by building the model in PyMC, sampling with NUTS, and using leave-one-out (LOO) crossvalidation for model selection.\nI originally came across a version of it in the first chapter of Andrew G. Wilson’s thesis. It builds on the example you’ve probably seen in textbooks. Given a simple scatterplot, fit a curve to it and use model selection to determine the right number of basis fucntios to use. It’s designed to demonstrate the bias variance tradeoff. Or, you can use the Bayesian evidence to select the right polynomial degree.\nTo rephrase Neal’s quote in practical terms, when you’re trying to model some unknown function, by trying to find the “right” number of basis functions you’re deliberately limiting the complexity of the model. The problem is that isn’t actually the goal. The real goal is to get a parsimonious representation of the unknown function – no one is asking you, directly at least, about the best number of basis vectors you should use to do that. It would be better instead to use a complex model of which the simple model is a special case, and let the data decide. At the risk of simplifying too much, the reason people are interested in Gaussian processes is because they are really good at doing this automatically."
  },
  {
    "objectID": "occams-razor.html#the-two-experiments",
    "href": "occams-razor.html#the-two-experiments",
    "title": "Putting priors on model complexity, fix title not about PC priors",
    "section": "The two experiments",
    "text": "The two experiments\nWe’re going to run two experiments. First, and you’ve probably seen before, we’re going to build a sequence of polynomial regression models with increasing complexity and then choose the “best” model, according to the leave-one-out crossvalidation (LOO-CV) metric. We’ll use the exact same priors for the polynomial coefficients.\nThen in the second experiment, we’ll do the exact same thing, except, we’ll keep decreasing the prior scale we use for the polynomial coefficients as we add more terms. We’re effectively using our priors as a hedge against overfitting. Then we’ll see what LOO-CV says is the best model, and compare the results to the first experiment.\nFor our toy example, here’s some data that’s generated from a step function with a bit of additive Gaussian noise.\n\nx = np.linspace(0, 2, 50)\nf = np.ones(50)\nf[:25] = 0.0\ny = f + 0.3 * rng.normal(size=len(x))\n\nplt.plot(x[:25], f[:25], color='k', label=\"underlying function\")\nplt.plot(x[25:], f[25:], color='k')\nplt.plot(x, y, 'o', label=\"observed data\");\nplt.xlim([0, 2]);\nplt.xlabel(\"x\");\nplt.ylabel(\"y\");\nplt.legend(loc=\"lower right\");\nplt.title(\"Our toy dataset\");\n\n\n\n\n\n\n\n\n\nDetails\nBefore getting going and actually modeling this, there’s one preliminary thing to mention. Feel free to skip over this bit about the QR decomposition because it’s not really core to the goal here. We just need it to make our priors on the polynomial coefficients comparable and on the same overall scale.\nWe’ll use the QR decomposition to orthogonalize our basis. I found some Python code to do this here, pasted below.\n\ndef ortho_poly_fit(x, degree = 1):\n    n = degree + 1\n    x = np.asarray(x).flatten()\n    if(degree &gt;= len(np.unique(x))):\n            stop(\"'degree' must be less than number of unique points\")\n    xbar = np.mean(x)\n    x = x - xbar\n    X = np.vander(x, n, increasing=True)\n    q,r = np.linalg.qr(X)\n\n    z = np.diag(np.diag(r))\n    raw = np.dot(q, z)\n\n    norm2 = np.sum(raw**2, axis=0)\n    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]\n    Z = raw / np.sqrt(norm2)\n    return Z, norm2, alpha\n\nThe left panel below shows the first four polynomial basis vectors and gives their correlation matrix. The right panel shows the same four basis vectors after they’ve been orthogonalized.\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\ndegree = 3\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10.0, 4.0])\n\n## SUBPLOT 1\nX = np.vander(x, degree + 1, increasing=True)[:, :degree+1]\nax1.plot(x, X, lw=2);\nax1.set_xlim([0.0, 2.0])\nax1.set_ylim([-0.2, 8.0])\n\naxins1 = inset_axes(ax1, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nXcorr = np.corrcoef(X[:, 1:].T)\naxins1.set_xticks([])\naxins1.set_yticks([])\naxins1.imshow(Xcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Xcorr[j, i]\n        axins1.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"white\")\n\n## SUBPLOT 2\nZ, _, _ = ortho_poly_fit(x, degree=degree)\nax2.plot(x, Z, lw=2);\nax2.set_xlim([0.0, 2.0])\nax2.set_ylim([-0.5, 0.9])\n\naxins2 = inset_axes(ax2, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nZcorr = np.corrcoef(Z[:, 1:].T)\naxins2.set_xticks([])\naxins2.set_yticks([])\naxins2.imshow(Zcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Zcorr[j, i]\n        axins2.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"c\")\n\n\n\n\n\n\n\n\nThere are other benefits to orthogonalizing the basis, but our main reason is to have all the basis function coefficients living on the same scale. Notice in the left panel the basis functions range from [0, 2], [0, 4] and [0, 9], so scale-wise they’re all over the place. A lesser reason (for our purposes here) is that the basis functions are now orthogonal, which makes things quite a bit easier on the sampler. The two inset panels show the correlation matrix of the 3 non-constant basis vectors."
  },
  {
    "objectID": "occams-razor.html#bonus-points",
    "href": "occams-razor.html#bonus-points",
    "title": "Putting priors on model complexity, fix title not about PC priors",
    "section": "Bonus points",
    "text": "Bonus points\nWhat about gamma, how did we choose that? The Occam’s razor paper also talks about this (at the start of Sec. 3). This time we’ll just run it once as a 30 degree polynomial (which is only 5 less than the total number of data points!). We should get the same ELPD. Yes, this is how I chose the magic number gamma = 1.25 above.\n\nwith pm.Model() as model:\n    degree = 30\n    Z, _, _ = ortho_poly_fit(x, degree=degree)\n   \n    gamma = pm.Exponential(\"gamma\", scale=1.0)\n    scale = 5.0 * np.arange(1, degree + 2) ** (-gamma)\n    beta = pm.Normal(\"beta\", mu=0.0, sigma=scale, size=degree + 1)\n   \n    mu = pm.Deterministic(\"mu\", Z @ beta)\n    sigma = pm.Exponential(\"sigma\", scale=5.0)\n    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n        \n    idata = pm.sample(nuts_sampler=\"nutpie\", target_accept=0.9)\n    pm.sample_posterior_predictive(idata, progressbar=False, extend_inferencedata=True)\n    pm.compute_log_likelihood(idata, progressbar=False)\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00  Chains in warmup: 0, Divergences: 0]\n    \n    \n\n\nSampling: [y]\n\n\n\naz.plot_trace(idata, var_names=\"gamma\");\n\n\n\n\n\n\n\n\n\nelpd_data = az.loo(idata)\nprint(f\"ELPD: {elpd_data.elpd_loo:0.2f}, ELPD SE: {elpd_data.se:0.2f}\")\n\nELPD: -14.71, ELPD SE: 5.88\n\n\n\nall_idatas = {**idata_results1, **idata_results2, **{\"Prior on gamma\": idata}}\n\n\naz.compare(all_idatas).head(10)\n\n/Users/andre/miniconda3/envs/dev/lib/python3.11/site-packages/arviz/stats/stats.py:309: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'True' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n/Users/andre/miniconda3/envs/dev/lib/python3.11/site-packages/arviz/stats/stats.py:309: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'log' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nPrior on gamma\n0\n-14.713950\n11.646827\n0.000000\n0.614029\n5.884088\n0.000000\nTrue\nlog\n\n\nExp 2, degree 7\n1\n-17.903478\n7.316925\n3.189529\n0.272241\n5.682735\n8.401857\nFalse\nlog\n\n\nExp 1, degree 7\n2\n-17.990226\n8.009275\n3.276277\n0.113730\n5.163729\n8.268976\nTrue\nlog\n\n\nExp 2, degree 9\n3\n-18.163848\n7.680631\n3.449898\n0.000000\n5.568024\n8.282049\nFalse\nlog\n\n\nExp 2, degree 8\n4\n-18.203050\n7.603282\n3.489101\n0.000000\n5.602184\n8.322322\nTrue\nlog\n\n\nExp 2, degree 12\n5\n-18.206949\n8.157549\n3.493000\n0.000000\n5.197726\n8.023377\nTrue\nlog\n\n\nExp 2, degree 11\n6\n-18.276150\n8.165427\n3.562200\n0.000000\n5.300492\n8.106990\nTrue\nlog\n\n\nExp 2, degree 10\n7\n-18.485222\n8.217779\n3.771272\n0.000000\n5.498343\n8.270510\nTrue\nlog\n\n\nExp 2, degree 27\n8\n-18.530088\n10.165056\n3.816139\n0.000000\n5.392681\n8.231695\nFalse\nlog\n\n\nExp 2, degree 19\n9\n-18.534833\n9.554110\n3.820883\n0.000000\n5.349856\n8.187055\nFalse\nlog\n\n\n\n\n\n\n\n\nThis shows that putting a prior on gamma actually gives the best model, by far."
  },
  {
    "objectID": "occams-razor.html#summary",
    "href": "occams-razor.html#summary",
    "title": "Putting priors on model complexity, fix title not about PC priors",
    "section": "Summary",
    "text": "Summary\nWe’ve essentially transferred our “prior” on model complexity from a discrete problem via the choice of the number of basis vectors, to a non-scare-quoted continuous prior on model complexity, represented by the gamma parameter. This example nicely demonstrates that the common example you’ve probably seen about model complexity and the bias variance trade-off isn’t really the whole story. I hope this gives a concrete example of the quote\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nI think it’s kind of interesting that Bayesian regression with normal priors on the coefficients is equivalent to ridge regression, where the scale you use on all the priors is the L2 penalty \\(\\lambda\\). Does it make sense to do ridge regression where successive coefficients get penalized more and more by increasing \\(\\lambda\\)?\nTheres also a direct conceptual connection to Gaussian processes, which can be seen as models that use an infinite number of basis vectors (explained in chapter 2 of R+W’s GPML book). Those infinite number of basis numbers have very particular priors though, which for Matern family GPs, model complexity is directly controlled by the lengthscale parameter.\nThere’s also a relationship to penalized splines. There’s a nice case study in Stan and an implementation here in PyMC. The same underlying concept is at play here too. The idea is not to spend too much time choosing the number and location of knots, but instead choose “too many” knots and rely on a smoothing prior on the coefficients to control model complexity.\nFor practical problems, it means you don’t need to do model selection for these types of problems, you just need a model that can handle the complexity of the data and carefully choose your priors."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Choosing a prior for the lengthscale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPriors, polynomial regressions, and ‘Occam’s Razor’\n\n\n\n\n\n\nbayesian\n\n\n\n\n\n\n\n\n\nJun 23, 2024\n\n\nBill Engels\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What about this blog"
  },
  {
    "objectID": "posts/what-is-a-GP/how-to-fit-a-GP.html",
    "href": "posts/what-is-a-GP/how-to-fit-a-GP.html",
    "title": "Bill Engels' Web Log",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport pytensor.tensor as pt\nimport arviz as az\n\nThis will be about how to fit GPs effectively using MCMC, where how you choose you’re priors is really important. In “GP only” packages like GPflow, GPyTorch or GPy that rely more variational inference or type 2 maximum likelihood, prior choice is still important, but isn’t as critical for success. The reason is because you only need to find a single solution for the hyperparameters, usually the lengthscales. With MCMC, we’re averaging over all possible lengthscale choices.\n\nChoosing a prior for the lengthscale\nChoosing a prior for the lengthscale is really important. In a lot of Bayesian models you can more often than not get away with using weakly informative priors, like Gaussians with large variances for regression coeficients, or half-normals for standard deviation parameters, and trust that your model will give reasonable results. With GPs this isn’t the case, you’ll have to pause and think carefully about it for every model. There’s a good amount of research on the topic, so there are guidelines we can follow. I think most people hit a wall when they’re working with GPs at this point. After hearing about how great GPs are and learning the basics, when they apply them to a problem they choose poor priors or they don’t fully understanding what their prior choices are actually implying. Fortunately, it doesn’t require a ton of math to understand the failure modes, and it’s really not that complicated once you walk through it.\nIn this article we’ll walk through these failure modes and provide guidelines for setting the priors on Gaussian processes that use Matern covariances. Before digging into that, the TLDR on guidelines for setting priors on GP parameters is: 1. The PC prior. This is defined for one, two, or three dimensional problems (like timeseries, spatial, and spatio-temporal problems). This prior makes sense if you’re not sure a GP is present in your data and you’re trying to detect one. 2. The inverse gamma for the lengthscale. The theoretical basis for this prior is a bit less developed than the PC prior, but it has nice properties that make it usually work well. In fact, the PC prior in two dimensions is the inverse gamma. 3. For the scale parameter \\(\\eta\\) I prefer the Exponential prior. First it’s part of the PC prior, and second it has a longer tail than something like a half-normal. This choice isn’t as critical as the lengthscale prior though.\nBefore explaining why these priors work well we’ll start using them first, while we work through the GP sampling failure modes. While we work through the GP failure modes, we’ll explain why these priors work well ahd how they fix some of these problems.\n\nReason 1. Identifiability issues between the GP and an intercept\nIn order to estimate the parameters of any model, Bayesian or not, you need to be sure that they are identifiabile. For instance, say you are trying to estimate the mean \\(\\mu\\), of a list of numbers \\(y\\). The mean is perfectly identifiable. But, now imagine that for some reason you parameterized the mean \\(\\mu\\) as the sum of two unknown parameters, \\(\\theta_1\\) and \\(\\theta_2\\), so your model was \\(\\mu = \\theta_1 + \\theta_2\\). You can still estimate the mean \\(\\mu\\) perfectly, but \\(\\theta_1\\) and \\(\\theta_2\\) are not identifiable. You could add a constant to one, and subtract it from the other, and \\(\\mu\\) would be the same. GPs embedded in larger models often have identifiability issues, and it can sometimes depend on the lengthscale prior.\nThe problem can show up in two situations: 1. With a large lengthscale, the GP component can become confounded with the intercept in your model.\n2. The relative scale of latent function the GP is trying to capture and the intercept, are similar.\nTake the data below. The underlying function \\(f(x)\\) that we want to model with a GP is shown in blue. The observed data \\(y\\), in black, is just this function with some added Gaussian noise, to keep things simple. You’ll see that it oscillates, but not around zero. Instead it oscillates around 100.\n\nn = 60\nx = 10 * np.sort(np.random.rand(n))\nf_true = 0.1 + np.sin(2 * np.pi * x * 0.07)\ny = f_true + 0.25 * np.random.randn(n)\n\nplt.plot(x, f_true);\nplt.scatter(x, y, color=\"k\");\n\n\n\n\n\n\n\n\n\nwith pm.Model() as model:\n    eta = pm.Exponential(\"eta\", scale=1.0)\n    ell = pm.InverseGamma(\"ell\", mu=3, sigma=2)\n    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n    gp = pm.gp.Latent(cov_func=cov)\n\n    intercept = pm.Normal(\"intercept\", mu=0.0, sigma=10)\n    f = gp.prior(\"f\", X=x[:, None])\n\n    sigma = pm.HalfNormal(\"sigma\", sigma=5.0)\n    pm.Normal(\"y\", mu=intercept + f, sigma=sigma, observed=y)\n\nwith model:\n    idata = pm.sample(nuts_sampler=\"numpyro\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere were 70 divergences after tuning. Increase `target_accept` or reparameterize.\n\n\n\nf = az.extract(idata, var_names=\"f\")\n#intercept = az.extract(idata, var_names=\"intercept\")\n\n\nintercept\n\n0.0\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\nintercept = 0.0\npm.gp.util.plot_gp_dist(ax=ax, samples=(f + intercept).data.T, x=x);\nplt.scatter(x, y, color=\"k\");"
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html",
    "href": "posts/occams-razor/occams-razor.html",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "",
    "text": "import pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nseed = 12345678\nrng = np.random.default_rng(seed)\nI saw this quote in Gelman’s blog recently that reminded me of a cool example I saw awhile back from this short paper titled “Occam’s razor” from NIPS 2000. The example demonstrated that fitting an unknown function using a large number of basis function with certain regularizing priors was preferable to casting the problem as model selection and choosing the optimal number of basis vectors. It helped it click for me why non-parametric model components like Gaussian processes often work better than parametric models with a fixed set of features like polynomials or splines. The quote (from Radford Neal in the context of Bayesian neural networks) is:\nThe goal of this post is to reproduce that example in a modern context by building the model in PyMC, sampling with NUTS, and using leave-one-out (LOO) crossvalidation for model selection.\nYou’ve probably seen an example like this in textbooks: Given a simple scatterplot, fit a curve to it and use model selection to determine the right number of basis functions to use. You’re usually choosing the best polynomial order. The example is designed to demonstrate the bias variance tradeoff. Or alternatively you can use the Bayesian evidence to select the right polynomial degree.\nI think what Neal is saying is that it’s a mistake to transform the original problem into a new one, where the new goal is to find the “right” number of basis functions. It’s better to address the original problem directly, where the goal is to get a parsimonious representation of the unknown function. No one ever actually cares about the best number of basis vectors to use. Transforming the problem creates an unnecessary step."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#the-two-experiments",
    "href": "posts/occams-razor/occams-razor.html#the-two-experiments",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "The two experiments",
    "text": "The two experiments\nWe’re going to run two experiments. First, and you’ve probably seen before, we’re going to build a sequence of polynomial regression models with increasing complexity and then choose the “best” model, according to the leave-one-out crossvalidation (LOO-CV) metric. We’ll use the exact same priors for the polynomial coefficients.\nThen in the second experiment, we’ll do the exact same thing, except, we’ll keep decreasing the prior scale we use for the polynomial coefficients as we add more terms. Instead of the number of basis vectors, we’re using our priors as the hedge against overfitting. Then we’ll see what LOO-CV says is the best model.\nFor our experiment, here’s some data that’s generated from a step function with a bit of additive Gaussian noise.\n\nx = np.linspace(0, 2, 50)\nf = np.ones(50)\nf[:25] = 0.0\ny = f + 0.3 * rng.normal(size=len(x))\n\nplt.plot(x[:25], f[:25], color='k', label=\"underlying function\")\nplt.plot(x[25:], f[25:], color='k')\nplt.plot(x, y, 'o', label=\"observed data\");\nplt.xlim([0, 2]);\nplt.xlabel(\"x\");\nplt.ylabel(\"y\");\nplt.legend(loc=\"lower right\");\nplt.title(\"Our example dataset\");\n\n\n\n\n\n\n\n\n\nDetails\nBefore getting going and actually modeling this, there’s one preliminary thing to mention. Feel free to skip over this bit about the QR decomposition because it’s not really core to the goal here. We just need it to make our priors on the polynomial coefficients comparable and on the same overall scale.\nWe’ll use the QR decomposition to orthogonalize our basis. I found some Python code to do this here, pasted below.\n\ndef ortho_poly_fit(x, degree = 1):\n    n = degree + 1\n    x = np.asarray(x).flatten()\n    if(degree &gt;= len(np.unique(x))):\n            stop(\"'degree' must be less than number of unique points\")\n    xbar = np.mean(x)\n    x = x - xbar\n    X = np.vander(x, n, increasing=True)\n    q,r = np.linalg.qr(X)\n\n    z = np.diag(np.diag(r))\n    raw = np.dot(q, z)\n\n    norm2 = np.sum(raw**2, axis=0)\n    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]\n    Z = raw / np.sqrt(norm2)\n    return Z, norm2, alpha\n\nThe left panel below shows the first four polynomial basis vectors and gives their correlation matrix. The right panel shows the same four basis vectors after they’ve been orthogonalized.\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\ndegree = 3\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10.0, 4.0])\n\n## SUBPLOT 1\nX = np.vander(x, degree + 1, increasing=True)[:, :degree+1]\nax1.plot(x, X, lw=2);\nax1.set_xlim([0.0, 2.0])\nax1.set_ylim([-0.2, 8.0])\n\naxins1 = inset_axes(ax1, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nXcorr = np.corrcoef(X[:, 1:].T)\naxins1.set_xticks([])\naxins1.set_yticks([])\naxins1.imshow(Xcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Xcorr[j, i]\n        axins1.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"white\")\n\n## SUBPLOT 2\nZ, _, _ = ortho_poly_fit(x, degree=degree)\nax2.plot(x, Z, lw=2);\nax2.set_xlim([0.0, 2.0])\nax2.set_ylim([-0.5, 0.9])\n\naxins2 = inset_axes(ax2, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nZcorr = np.corrcoef(Z[:, 1:].T)\naxins2.set_xticks([])\naxins2.set_yticks([])\naxins2.imshow(Zcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Zcorr[j, i]\n        axins2.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"c\")\n\n\n\n\n\n\n\n\nThere are other benefits to orthogonalizing the basis, but our main reason is to have all the basis function coefficients living on the same scale. Notice in the left panel the basis functions range from [0, 2], [0, 4] and [0, 9], so scale-wise they’re all over the place. A lesser reason (for our purposes here) is that the basis functions are now orthogonal, which makes things quite a bit easier on the sampler. The two inset panels show the correlation matrix of the 3 non-constant basis vectors."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#bonus-points",
    "href": "posts/occams-razor/occams-razor.html#bonus-points",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "Bonus points",
    "text": "Bonus points\nWhat about gamma, how did we choose that? The Occam’s razor paper also talks about this (at the start of Sec. 3). This time we’ll just run it once as a 30 degree polynomial (which is only 5 less than the total number of data points!). We should get the same ELPD. Yes, this is how I chose the magic number gamma = 1.25 above.\n\nwith pm.Model() as model:\n    degree = 30\n    Z, _, _ = ortho_poly_fit(x, degree=degree)\n   \n    gamma = pm.Exponential(\"gamma\", scale=1.0)\n    scale = 5.0 * np.arange(1, degree + 2) ** (-gamma)\n    beta = pm.Normal(\"beta\", mu=0.0, sigma=scale, size=degree + 1)\n   \n    mu = pm.Deterministic(\"mu\", Z @ beta)\n    sigma = pm.Exponential(\"sigma\", scale=5.0)\n    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n        \n    idata = pm.sample(nuts_sampler=\"nutpie\", target_accept=0.9)\n    pm.sample_posterior_predictive(idata, progressbar=False, extend_inferencedata=True)\n    pm.compute_log_likelihood(idata, progressbar=False)\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00  Chains in warmup: 0, Divergences: 0]\n    \n    \n\n\nSampling: [y]\n\n\n\naz.plot_trace(idata, var_names=\"gamma\");\n\n\n\n\n\n\n\n\n\nelpd_data = az.loo(idata)\nprint(f\"ELPD: {elpd_data.elpd_loo:0.2f}, ELPD SE: {elpd_data.se:0.2f}\")\n\nELPD: -14.71, ELPD SE: 5.88\n\n\n\nall_idatas = {**idata_results1, **idata_results2, **{\"Prior on gamma\": idata}}\n\n\naz.compare(all_idatas).head(10)\n\n/Users/andre/miniconda3/envs/dev/lib/python3.11/site-packages/arviz/stats/stats.py:309: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'True' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n/Users/andre/miniconda3/envs/dev/lib/python3.11/site-packages/arviz/stats/stats.py:309: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'log' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nPrior on gamma\n0\n-14.713950\n11.646827\n0.000000\n0.614029\n5.884088\n0.000000\nTrue\nlog\n\n\nExp 2, degree 7\n1\n-17.903478\n7.316925\n3.189529\n0.272241\n5.682735\n8.401857\nFalse\nlog\n\n\nExp 1, degree 7\n2\n-17.990226\n8.009275\n3.276277\n0.113730\n5.163729\n8.268976\nTrue\nlog\n\n\nExp 2, degree 9\n3\n-18.163848\n7.680631\n3.449898\n0.000000\n5.568024\n8.282049\nFalse\nlog\n\n\nExp 2, degree 8\n4\n-18.203050\n7.603282\n3.489101\n0.000000\n5.602184\n8.322322\nTrue\nlog\n\n\nExp 2, degree 12\n5\n-18.206949\n8.157549\n3.493000\n0.000000\n5.197726\n8.023377\nTrue\nlog\n\n\nExp 2, degree 11\n6\n-18.276150\n8.165427\n3.562200\n0.000000\n5.300492\n8.106990\nTrue\nlog\n\n\nExp 2, degree 10\n7\n-18.485222\n8.217779\n3.771272\n0.000000\n5.498343\n8.270510\nTrue\nlog\n\n\nExp 2, degree 27\n8\n-18.530088\n10.165056\n3.816139\n0.000000\n5.392681\n8.231695\nFalse\nlog\n\n\nExp 2, degree 19\n9\n-18.534833\n9.554110\n3.820883\n0.000000\n5.349856\n8.187055\nFalse\nlog\n\n\n\n\n\n\n\n\nThis shows that putting a prior on gamma actually gives the best model in this situation, by far."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#summary",
    "href": "posts/occams-razor/occams-razor.html#summary",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "Summary",
    "text": "Summary\nWe’ve essentially transformed a discrete prior on model complexity via the choice of the number of basis vectors, to a continuous prior on model complexity, represented by the gamma parameter. This example nicely demonstrates that the common example you’ve probably seen about model complexity and the bias variance trade-off isn’t really the whole story.\nNeal argues that it’s better to use a complex model where the simple model is a special case, and let the data decide. Gaussian processes are really good at doing this. In particular, the HSGP approximation looks almost exactly like this.\nThere’s also a relationship to penalized splines. There’s a nice case study in Stan and an implementation here in PyMC. The same underlying concept is at play here too. The idea is not to spend too much time choosing the number and location of knots, but instead choose “too many” knots and rely on the coefficient priors to control model complexity. For spline coefficients, we also care about smoothness, in addition to how they prior penalizes their distance from zero. Flatter functions will all have more similar spline coefficients."
  }
]