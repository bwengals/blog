[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Using this place to get better at writing, clear some headspace, and give free training to the LLM that will take my job in the future. I’m incredibly fortunate to work with the fine folks at PyMC Labs."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html",
    "href": "posts/occams-razor/occams-razor.html",
    "title": "The Bias-Variance tradeoff vs. Occam’s Razor",
    "section": "",
    "text": "import pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nseed = 12345678\nrng = np.random.default_rng(seed)\n\n# silence PyMC logging\nimport logging\nlogger = logging.getLogger(\"pymc\")\nlogger.handlers.clear()"
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#the-bias-variance-tradeoff",
    "href": "posts/occams-razor/occams-razor.html#the-bias-variance-tradeoff",
    "title": "The Bias-Variance tradeoff vs. Occam’s Razor",
    "section": "The bias / variance tradeoff",
    "text": "The bias / variance tradeoff\nThe bias / variance tradeoff is a tentpole of statistical learning theory. Here’s the wikipedia entry if you need a refresher. The gist of it is:\nAdding more parameters to a model increases its ability to fit the training data set which decreasing the overall bias of the model’s predictions. Then, if you were to resample and refit the training set, the model’s parameters will be more different each time, increasing the variance in that sense.\nThe usual advice that follows is to use model selection techniques to try and find a balance between the complexity of your model and the fit on your training data set. This blog post is about how to sidestep the model selection problem when we are in a Bayesian context by thinking carefully about our priors."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#the-subtle-part",
    "href": "posts/occams-razor/occams-razor.html#the-subtle-part",
    "title": "The Bias-Variance tradeoff vs. Occam’s Razor",
    "section": "The subtle part",
    "text": "The subtle part\nI had seen this quote in Gelman’s blog awhile back that reminded me of a cool example I saw years and years ago from this short paper titled “Occam’s razor” from NIPS 2000. Andrew Gordon Wilson also demonstrates this with a similar example in his thesis.\nThat paper showed that fitting an unknown function using a large number of basis function with particular regularizing priors was preferable to casting the problem as model selection and choosing the optimal number of basis vectors.\nThe example in that paper made it click for me why non-parametric model components like Gaussian processes often work better than parametric models with a fixed set of features like polynomials or splines. I also use this idea when I’m choosing which features – actual features, not basis functions – to consider adding into regression models. So the quote, which is from Radford Neal in the context of Bayesian neural networks, is:\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nThe goal of this post is to illustrate this idea by reproducing the example in the “Occam’s Razor” paper, but in a more modern software context using PyMC, sampling with NUTS, and finally using leave-one-out (LOO) crossvalidation for model selection.\nYou’ve probably seen an example like this in textbooks: Given a simple scatterplot, fit a curve to it and use model selection to determine the right number of basis functions to use. You’re usually choosing the best polynomial order. The example is designed to demonstrate the bias variance tradeoff. Or alternatively you can use the Bayesian evidence to select the right polynomial degree.\nI think what Neal is saying is that it’s a mistake to transform the original problem into a new one, where the new goal is to find the “right” number of basis functions. It’s better to address the original problem directly, where the goal is to get a parsimonious representation of the unknown function. Most likely no one will ever care about the best number of basis vectors to use. By turning your modeling problem into a model selection problem, you’ve made your work harder."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#details-details-details",
    "href": "posts/occams-razor/occams-razor.html#details-details-details",
    "title": "The Bias-Variance tradeoff vs. Occam’s Razor",
    "section": "Details, details, details",
    "text": "Details, details, details\nBefore getting going and actually modeling this there’s one preliminary thing to mention. Feel free to skip over this bit about the QR decomposition because it’s not really core to the goal here. We just need it to make our priors on the polynomial coefficients comparable and on the same overall scale.\nWe’ll use the QR decomposition to orthogonalize our basis. I found some Python code to do this here, pasted below.\n\ndef ortho_poly_fit(x, degree = 1):\n    n = degree + 1\n    x = np.asarray(x).flatten()\n    if(degree &gt;= len(np.unique(x))):\n            stop(\"'degree' must be less than number of unique points\")\n    xbar = np.mean(x)\n    x = x - xbar\n    X = np.vander(x, n, increasing=True)\n    q,r = np.linalg.qr(X)\n\n    z = np.diag(np.diag(r))\n    raw = np.dot(q, z)\n\n    norm2 = np.sum(raw**2, axis=0)\n    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]\n    Z = raw / np.sqrt(norm2)\n    return Z, norm2, alpha\n\nThe left panel below shows the first four polynomial basis vectors and gives their correlation matrix. The right panel shows the same four basis vectors after they’ve been orthogonalized.\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\ndegree = 3\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10.0, 4.0])\n\n## SUBPLOT 1\nX = np.vander(x, degree + 1, increasing=True)[:, :degree+1]\nax1.plot(x, X, lw=2);\nax1.set_xlim([0.0, 2.0])\nax1.set_ylim([-0.2, 8.0])\n\naxins1 = inset_axes(ax1, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nXcorr = np.corrcoef(X[:, 1:].T)\naxins1.set_xticks([])\naxins1.set_yticks([])\naxins1.imshow(Xcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Xcorr[j, i]\n        axins1.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"white\")\n\n## SUBPLOT 2\nZ, _, _ = ortho_poly_fit(x, degree=degree)\nax2.plot(x, Z, lw=2);\nax2.set_xlim([0.0, 2.0])\nax2.set_ylim([-0.5, 0.9])\n\naxins2 = inset_axes(ax2, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nZcorr = np.corrcoef(Z[:, 1:].T)\naxins2.set_xticks([])\naxins2.set_yticks([])\naxins2.imshow(Zcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Zcorr[j, i]\n        axins2.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"c\")\n\n\n\n\n\n\n\n\nThere are other benefits to orthogonalizing the basis, but our main reason is to have all the basis function coefficients living on the same scale. Notice in the left panel the basis functions range from [0, 2], [0, 4] and [0, 9], so scale-wise they’re all over the place. A lesser reason (for our purposes here) is that the basis functions are now orthogonal, which makes things quite a bit easier on the sampler. The two inset panels show the correlation matrix of the first 3 non-constant basis vectors."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TODO: cleverer title",
    "section": "",
    "text": "The Bias-Variance tradeoff vs. Occam’s Razor\n\n\n\n\n\n\nbayesian\n\n\n\nUsing priors to let the data choose model complexity instead of doing model selection\n\n\n\n\n\nMar 3, 2025\n\n\nBill Engels\n\n\n\n\n\n\nNo matching items"
  }
]