[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What about this blog"
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html",
    "href": "posts/occams-razor/occams-razor.html",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "",
    "text": "import pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nseed = 12345678\nrng = np.random.default_rng(seed)\n\nimport logging\nlogger = logging.getLogger(\"pymc\")\nlogger.handlers.clear()\nI had seen this quote in Gelman’s blog awhile back that reminded me of a cool example I saw years and years ago from this short paper titled “Occam’s razor” from NIPS 2000. Andrew Gordon Wilson also demonstrates this with a similar example in his thesis.\nThat paper showed that fitting an unknown function using a large number of basis function with particular regularizing priors was preferable to casting the problem as model selection and choosing the optimal number of basis vectors.\nThe example in that paper made it click for me why non-parametric model components like Gaussian processes often work better than parametric models with a fixed set of features like polynomials or splines. I also use this idea when I’m choosing which features – actual features, not basis functions – to consider adding into regression models. So the quote, which is from Radford Neal in the context of Bayesian neural networks, is:\nThe goal of this post is to illustrate this idea by reproducing the example in the “Occam’s Razor” paper, but in a more modern software context using PyMC, sampling with NUTS, and finally using leave-one-out (LOO) crossvalidation for model selection.\nYou’ve probably seen an example like this in textbooks: Given a simple scatterplot, fit a curve to it and use model selection to determine the right number of basis functions to use. You’re usually choosing the best polynomial order. The example is designed to demonstrate the bias variance tradeoff. Or alternatively you can use the Bayesian evidence to select the right polynomial degree.\nI think what Neal is saying is that it’s a mistake to transform the original problem into a new one, where the new goal is to find the “right” number of basis functions. It’s better to address the original problem directly, where the goal is to get a parsimonious representation of the unknown function. No one ever actually cares about the best number of basis vectors to use. Transforming the problem creates an unnecessary step."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#the-two-experiments",
    "href": "posts/occams-razor/occams-razor.html#the-two-experiments",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "The two experiments",
    "text": "The two experiments\nWe’re going to run two experiments. First, and you’ve probably seen before, we’re going to build a sequence of polynomial regression models with increasing complexity and then choose the “best” model, according to the leave-one-out crossvalidation (LOO-CV) metric. We’ll use the exact same priors for the polynomial coefficients.\nThen in the second experiment, we’ll do the exact same thing, except, we’ll keep decreasing the prior scale we use for the polynomial coefficients as we add more terms. Instead of the number of basis vectors, we’re using our priors as the hedge against overfitting. Then we’ll see what LOO-CV says is the best model.\nWe’ll use data generated from a step function with a bit of additive Gaussian noise for our two experiments. Notice that since we’re using polynomial basis functions, none of our proposed models will be correct. We’re in the M-open case.\n\nx = np.linspace(0, 2, 50)\nf = np.ones(50)\nf[:25] = 0.0\ny = f + 0.3 * rng.normal(size=len(x))\n\nplt.plot(x[:25], f[:25], color='k', label=\"underlying function\")\nplt.plot(x[25:], f[25:], color='k')\nplt.plot(x, y, 'o', label=\"observed data\");\nplt.xlim([0, 2]);\nplt.xlabel(\"x\");\nplt.ylabel(\"y\");\nplt.legend(loc=\"lower right\");\nplt.title(\"Our example dataset\");\n\n\n\n\n\n\n\n\n\nDetails\nBefore getting going and actually modeling this there’s one preliminary thing to mention. Feel free to skip over this bit about the QR decomposition because it’s not really core to the goal here. We just need it to make our priors on the polynomial coefficients comparable and on the same overall scale.\nWe’ll use the QR decomposition to orthogonalize our basis. I found some Python code to do this here, pasted below.\n\ndef ortho_poly_fit(x, degree = 1):\n    n = degree + 1\n    x = np.asarray(x).flatten()\n    if(degree &gt;= len(np.unique(x))):\n            stop(\"'degree' must be less than number of unique points\")\n    xbar = np.mean(x)\n    x = x - xbar\n    X = np.vander(x, n, increasing=True)\n    q,r = np.linalg.qr(X)\n\n    z = np.diag(np.diag(r))\n    raw = np.dot(q, z)\n\n    norm2 = np.sum(raw**2, axis=0)\n    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]\n    Z = raw / np.sqrt(norm2)\n    return Z, norm2, alpha\n\nThe left panel below shows the first four polynomial basis vectors and gives their correlation matrix. The right panel shows the same four basis vectors after they’ve been orthogonalized.\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\ndegree = 3\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10.0, 4.0])\n\n## SUBPLOT 1\nX = np.vander(x, degree + 1, increasing=True)[:, :degree+1]\nax1.plot(x, X, lw=2);\nax1.set_xlim([0.0, 2.0])\nax1.set_ylim([-0.2, 8.0])\n\naxins1 = inset_axes(ax1, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nXcorr = np.corrcoef(X[:, 1:].T)\naxins1.set_xticks([])\naxins1.set_yticks([])\naxins1.imshow(Xcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Xcorr[j, i]\n        axins1.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"white\")\n\n## SUBPLOT 2\nZ, _, _ = ortho_poly_fit(x, degree=degree)\nax2.plot(x, Z, lw=2);\nax2.set_xlim([0.0, 2.0])\nax2.set_ylim([-0.5, 0.9])\n\naxins2 = inset_axes(ax2, width=\"40%\", height=\"40%\", loc=2, borderpad=2)\nZcorr = np.corrcoef(Z[:, 1:].T)\naxins2.set_xticks([])\naxins2.set_yticks([])\naxins2.imshow(Zcorr, vmin=0, vmax=1, cmap=\"bone_r\");\nfor i in range(3):\n    for j in range(3):\n        c = Zcorr[j, i]\n        axins2.text(i, j, (\"%0.2f\" % c), va='center', ha='center', color=\"c\")\n\n\n\n\n\n\n\n\nThere are other benefits to orthogonalizing the basis, but our main reason is to have all the basis function coefficients living on the same scale. Notice in the left panel the basis functions range from [0, 2], [0, 4] and [0, 9], so scale-wise they’re all over the place. A lesser reason (for our purposes here) is that the basis functions are now orthogonal, which makes things quite a bit easier on the sampler. The two inset panels show the correlation matrix of the first 3 non-constant basis vectors."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#bonus-points",
    "href": "posts/occams-razor/occams-razor.html#bonus-points",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "Bonus points",
    "text": "Bonus points\nWhat about gamma, how did we choose that? The Occam’s razor paper also talks about this (at the start of Sec. 3). This time we’ll just run it once as a 30 degree polynomial (which is only 5 less than the total number of data points!). We should get the same ELPD. Yes, this is how I chose the magic number gamma = 1.25 above.\n\nwith pm.Model() as model:\n    degree = 30\n    Z, _, _ = ortho_poly_fit(x, degree=degree)\n   \n    gamma = pm.Exponential(\"gamma\", scale=1.0)\n    scale = 5.0 * np.arange(1, degree + 2) ** (-gamma)\n    beta = pm.Normal(\"beta\", mu=0.0, sigma=scale, size=degree + 1)\n   \n    mu = pm.Deterministic(\"mu\", Z @ beta)\n    sigma = pm.Exponential(\"sigma\", scale=5.0)\n    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n        \n    idata = pm.sample(nuts_sampler=\"nutpie\", target_accept=0.9)\n    pm.sample_posterior_predictive(idata, progressbar=False, extend_inferencedata=True)\n    pm.compute_log_likelihood(idata, progressbar=False)\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for now\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.21\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.35\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.37\n                    7\n                \n            \n                \n                    \n                        \n                        \n                    \n                    2000\n                    0\n                    0.37\n                    7\n                \n            \n            \n        \n    \n\n\n\n\naz.plot_trace(idata, var_names=\"gamma\");\n\n\n\n\n\n\n\n\n\nelpd_data = az.loo(idata)\nprint(f\"ELPD: {elpd_data.elpd_loo:0.2f}, ELPD SE: {elpd_data.se:0.2f}\")\n\nELPD: -19.55, ELPD SE: 5.65\n\n\n\nall_idatas = {**idata_results1, **idata_results2, **{\"Prior on gamma\": idata}}\n\n\nwarnings.simplefilter(\"ignore\", category=FutureWarning)\naz.compare(all_idatas).head(10)\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nExp 2, degree 7\n0\n-17.454417\n6.941462\n0.000000\n8.843849e-01\n5.637879\n0.000000\nTrue\nlog\n\n\nExp 2, degree 8\n1\n-17.889721\n7.308506\n0.435304\n8.432485e-11\n5.551900\n0.345881\nFalse\nlog\n\n\nExp 2, degree 9\n2\n-18.075383\n7.693926\n0.620966\n8.439987e-11\n5.598566\n0.523907\nFalse\nlog\n\n\nExp 1, degree 7\n3\n-18.110580\n8.180686\n0.656163\n9.358834e-02\n5.170638\n1.436110\nTrue\nlog\n\n\nExp 2, degree 14\n4\n-18.215979\n8.552262\n0.761563\n8.368721e-11\n5.167451\n1.050139\nTrue\nlog\n\n\nExp 2, degree 11\n5\n-18.249420\n8.155402\n0.795003\n8.424486e-11\n5.345232\n0.717376\nTrue\nlog\n\n\nExp 2, degree 16\n6\n-18.413651\n9.180511\n0.959234\n8.374915e-11\n5.385329\n1.087181\nTrue\nlog\n\n\nExp 2, degree 18\n7\n-18.417945\n9.394612\n0.963528\n8.369683e-11\n5.313482\n1.114191\nTrue\nlog\n\n\nExp 2, degree 12\n8\n-18.425259\n8.428294\n0.970842\n8.436344e-11\n5.245622\n0.767023\nTrue\nlog\n\n\nExp 2, degree 13\n9\n-18.448142\n8.545733\n0.993726\n8.429243e-11\n5.217694\n0.846622\nFalse\nlog\n\n\n\n\n\n\n\nIt looks like in this case putting a prior on gamma isn’t as good as choosing gamma = 1.25, however, notice that the ELPD estimates are all within the standard error of each other."
  },
  {
    "objectID": "posts/occams-razor/occams-razor.html#summary",
    "href": "posts/occams-razor/occams-razor.html#summary",
    "title": "Priors, polynomial regressions, and ‘Occam’s Razor’",
    "section": "Summary",
    "text": "Summary\nWe’ve essentially transformed a discrete prior on model complexity via the choice of the number of basis vectors, to a continuous prior on model complexity, represented by the gamma parameter. This example nicely demonstrates that the common example you’ve probably seen about model complexity and the bias variance trade-off isn’t really the whole story.\nNeal argues that it’s better to use a complex model where the simple model is a special case, and let the data decide. Gaussian processes are really good at doing this. In particular, the HSGP approximation looks almost exactly like this.\nThere’s also a relationship to penalized splines. There’s a nice case study in Stan and an implementation here in PyMC. The same underlying concept is at play here too. The idea is not to spend too much time choosing the number and location of knots, but instead choose “too many” knots and rely on the coefficient priors to control model complexity. For spline coefficients, we also care about smoothness, in addition to how they prior penalizes their distance from zero. Flatter functions will all have more similar spline coefficients.\nI also think this same idea shows up when choosing features to include in a regression model. I think it’s too easy to get yourself wrapped around the handle of choosing regressors, turning your modeling problem into model selection problem. This example shows that it’s preferable to have a more complex model available to the data, but to allow the data to choose a simpler model in the absence of evidence for a more complex model.\nAs a resolution for this example, it doesn’t really matter if we choose the prior from experiment 1 and select 7 basis vectors, or take the prior from experiment 2 and select a larger number. It’s nice to know that in the context of more complicated models, as long as we don’t have identifiability issues, the approach of experiment 2 is both easier to implement and is more likely to yield better models."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Priors, polynomial regressions, and ‘Occam’s Razor’\n\n\n\n\n\n\nbayesian\n\n\n\n\n\n\n\n\n\nMar 3, 2025\n\n\nBill Engels\n\n\n\n\n\n\nNo matching items"
  }
]